{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train, test loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-01T03:49:49.600156Z",
     "start_time": "2020-12-01T03:49:11.155008Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading train...done\n",
      "x_train shape: (131302, 5000, 2)\n",
      "x_test.shape: (13749,)\n"
     ]
    }
   ],
   "source": [
    "dataset_path = 'dataset/preprocess2/input3-2/'\n",
    "\n",
    "print('loading train...', flush=True, end='')\n",
    "\n",
    "# x를 loading해서 (batch_size, step, channel)\n",
    "x_train = np.load(dataset_path+'x_train.npz', allow_pickle=True)['arr_0']\n",
    "x_test = np.load(dataset_path+'x_test.npz', allow_pickle=True)['arr_0']\n",
    "y_train = np.load(dataset_path+'y_train.npz')['arr_0']\n",
    "y_test = np.load(dataset_path+'y_test.npz')['arr_0']\n",
    "print('done', flush=True)\n",
    "\n",
    "# binary classification\n",
    "y_train_bin = y_train >= 5\n",
    "y_test_bin = y_test >= 5\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test.shape:', y_test.shape)\n",
    "\n",
    "\n",
    "# random shuffling\n",
    "ids = np.arange(x_train.shape[0])\n",
    "np.random.shuffle(ids)\n",
    "\n",
    "x_train = x_train[ids]\n",
    "y_train = y_train[ids]\n",
    "y_train_bin = y_train_bin[ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-01T03:52:00.622Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64-64\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/multi_device_iterator_ops.py:601: get_next_as_optional (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Iterator.get_next_as_optional()` instead.\n",
      "INFO:tensorflow:batch_all_reduce: 10 all-reduces with algorithm = nccl, num_packs = 1\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:batch_all_reduce: 10 all-reduces with algorithm = nccl, num_packs = 1\n",
      "462/462 [==============================] - ETA: 0s - loss: 0.8572 - acc: 0.5836 - auc: 0.6167\n",
      "Epoch 00001: val_loss improved from inf to 0.71398, saving model to output/1D_CNN_model_bin_conv64_conv64_filtersize10_relu_bn_maxpool2_globalmaxpool_dropout0.3_dense0_dropout0.2_batch256_lr_0.002/weights.hdf5\n",
      "462/462 [==============================] - 18s 40ms/step - loss: 0.8572 - acc: 0.5836 - auc: 0.6167 - val_loss: 0.7140 - val_acc: 0.6566 - val_auc: 0.7171\n",
      "Epoch 2/100\n",
      "462/462 [==============================] - ETA: 0s - loss: 0.5901 - acc: 0.6462 - auc: 0.7014\n",
      "Epoch 00002: val_loss improved from 0.71398 to 0.57775, saving model to output/1D_CNN_model_bin_conv64_conv64_filtersize10_relu_bn_maxpool2_globalmaxpool_dropout0.3_dense0_dropout0.2_batch256_lr_0.002/weights.hdf5\n",
      "462/462 [==============================] - 14s 31ms/step - loss: 0.5901 - acc: 0.6462 - auc: 0.7014 - val_loss: 0.5778 - val_acc: 0.5942 - val_auc: 0.7222\n",
      "Epoch 3/100\n",
      "462/462 [==============================] - ETA: 0s - loss: 0.5726 - acc: 0.6578 - auc: 0.7061\n",
      "Epoch 00003: val_loss improved from 0.57775 to 0.56762, saving model to output/1D_CNN_model_bin_conv64_conv64_filtersize10_relu_bn_maxpool2_globalmaxpool_dropout0.3_dense0_dropout0.2_batch256_lr_0.002/weights.hdf5\n",
      "462/462 [==============================] - 14s 31ms/step - loss: 0.5726 - acc: 0.6578 - auc: 0.7061 - val_loss: 0.5676 - val_acc: 0.6865 - val_auc: 0.7218\n",
      "Epoch 4/100\n",
      "462/462 [==============================] - ETA: 0s - loss: 0.5627 - acc: 0.6667 - auc: 0.7096\n",
      "Epoch 00004: val_loss did not improve from 0.56762\n",
      "462/462 [==============================] - 14s 31ms/step - loss: 0.5627 - acc: 0.6667 - auc: 0.7096 - val_loss: 0.5870 - val_acc: 0.6835 - val_auc: 0.7150\n",
      "Epoch 5/100\n",
      "462/462 [==============================] - ETA: 0s - loss: 0.5621 - acc: 0.6692 - auc: 0.7074\n",
      "Epoch 00005: val_loss improved from 0.56762 to 0.56227, saving model to output/1D_CNN_model_bin_conv64_conv64_filtersize10_relu_bn_maxpool2_globalmaxpool_dropout0.3_dense0_dropout0.2_batch256_lr_0.002/weights.hdf5\n",
      "462/462 [==============================] - 14s 31ms/step - loss: 0.5621 - acc: 0.6692 - auc: 0.7074 - val_loss: 0.5623 - val_acc: 0.6506 - val_auc: 0.7022\n",
      "Epoch 6/100\n",
      "462/462 [==============================] - ETA: 0s - loss: 0.5583 - acc: 0.6749 - auc: 0.7122\n",
      "Epoch 00006: val_loss improved from 0.56227 to 0.55604, saving model to output/1D_CNN_model_bin_conv64_conv64_filtersize10_relu_bn_maxpool2_globalmaxpool_dropout0.3_dense0_dropout0.2_batch256_lr_0.002/weights.hdf5\n",
      "462/462 [==============================] - 14s 31ms/step - loss: 0.5583 - acc: 0.6749 - auc: 0.7122 - val_loss: 0.5560 - val_acc: 0.6657 - val_auc: 0.7256\n",
      "Epoch 7/100\n",
      "461/462 [============================>.] - ETA: 0s - loss: 0.5540 - acc: 0.6762 - auc: 0.7134\n",
      "Epoch 00007: val_loss improved from 0.55604 to 0.54824, saving model to output/1D_CNN_model_bin_conv64_conv64_filtersize10_relu_bn_maxpool2_globalmaxpool_dropout0.3_dense0_dropout0.2_batch256_lr_0.002/weights.hdf5\n",
      "462/462 [==============================] - 14s 31ms/step - loss: 0.5540 - acc: 0.6762 - auc: 0.7134 - val_loss: 0.5482 - val_acc: 0.6892 - val_auc: 0.7319\n",
      "Epoch 8/100\n",
      "461/462 [============================>.] - ETA: 0s - loss: 0.5529 - acc: 0.6785 - auc: 0.7136\n",
      "Epoch 00008: val_loss did not improve from 0.54824\n",
      "462/462 [==============================] - 14s 31ms/step - loss: 0.5529 - acc: 0.6784 - auc: 0.7136 - val_loss: 0.5732 - val_acc: 0.5659 - val_auc: 0.7216\n",
      "Epoch 9/100\n",
      "461/462 [============================>.] - ETA: 0s - loss: 0.5509 - acc: 0.6794 - auc: 0.7149\n",
      "Epoch 00009: val_loss improved from 0.54824 to 0.54744, saving model to output/1D_CNN_model_bin_conv64_conv64_filtersize10_relu_bn_maxpool2_globalmaxpool_dropout0.3_dense0_dropout0.2_batch256_lr_0.002/weights.hdf5\n",
      "462/462 [==============================] - 14s 31ms/step - loss: 0.5509 - acc: 0.6794 - auc: 0.7148 - val_loss: 0.5474 - val_acc: 0.6917 - val_auc: 0.7264\n",
      "Epoch 10/100\n",
      "461/462 [============================>.] - ETA: 0s - loss: 0.5479 - acc: 0.6807 - auc: 0.7168\n",
      "Epoch 00010: val_loss did not improve from 0.54744\n",
      "462/462 [==============================] - 14s 31ms/step - loss: 0.5479 - acc: 0.6806 - auc: 0.7168 - val_loss: 0.5966 - val_acc: 0.5987 - val_auc: 0.7266\n",
      "Epoch 11/100\n",
      "461/462 [============================>.] - ETA: 0s - loss: 0.5447 - acc: 0.6838 - auc: 0.7205\n",
      "Epoch 00011: val_loss improved from 0.54744 to 0.54049, saving model to output/1D_CNN_model_bin_conv64_conv64_filtersize10_relu_bn_maxpool2_globalmaxpool_dropout0.3_dense0_dropout0.2_batch256_lr_0.002/weights.hdf5\n",
      "462/462 [==============================] - 14s 31ms/step - loss: 0.5447 - acc: 0.6838 - auc: 0.7205 - val_loss: 0.5405 - val_acc: 0.6943 - val_auc: 0.7375\n",
      "Epoch 12/100\n",
      "461/462 [============================>.] - ETA: 0s - loss: 0.5426 - acc: 0.6856 - auc: 0.7219\n",
      "Epoch 00012: val_loss improved from 0.54049 to 0.53332, saving model to output/1D_CNN_model_bin_conv64_conv64_filtersize10_relu_bn_maxpool2_globalmaxpool_dropout0.3_dense0_dropout0.2_batch256_lr_0.002/weights.hdf5\n",
      "462/462 [==============================] - 14s 31ms/step - loss: 0.5426 - acc: 0.6856 - auc: 0.7219 - val_loss: 0.5333 - val_acc: 0.6849 - val_auc: 0.7331\n",
      "Epoch 13/100\n",
      "461/462 [============================>.] - ETA: 0s - loss: 0.5416 - acc: 0.6863 - auc: 0.7228\n",
      "Epoch 00013: val_loss improved from 0.53332 to 0.53252, saving model to output/1D_CNN_model_bin_conv64_conv64_filtersize10_relu_bn_maxpool2_globalmaxpool_dropout0.3_dense0_dropout0.2_batch256_lr_0.002/weights.hdf5\n",
      "462/462 [==============================] - 14s 31ms/step - loss: 0.5416 - acc: 0.6862 - auc: 0.7227 - val_loss: 0.5325 - val_acc: 0.6928 - val_auc: 0.7408\n",
      "Epoch 14/100\n",
      "461/462 [============================>.] - ETA: 0s - loss: 0.5420 - acc: 0.6855 - auc: 0.7235\n",
      "Epoch 00014: val_loss did not improve from 0.53252\n",
      "462/462 [==============================] - 14s 31ms/step - loss: 0.5420 - acc: 0.6855 - auc: 0.7235 - val_loss: 0.5398 - val_acc: 0.6938 - val_auc: 0.7416\n",
      "Epoch 15/100\n",
      "461/462 [============================>.] - ETA: 0s - loss: 0.5392 - acc: 0.6869 - auc: 0.7268\n",
      "Epoch 00015: val_loss improved from 0.53252 to 0.52910, saving model to output/1D_CNN_model_bin_conv64_conv64_filtersize10_relu_bn_maxpool2_globalmaxpool_dropout0.3_dense0_dropout0.2_batch256_lr_0.002/weights.hdf5\n",
      "462/462 [==============================] - 14s 31ms/step - loss: 0.5392 - acc: 0.6869 - auc: 0.7268 - val_loss: 0.5291 - val_acc: 0.6971 - val_auc: 0.7427\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/100\n",
      "461/462 [============================>.] - ETA: 0s - loss: 0.5379 - acc: 0.6886 - auc: 0.7311\n",
      "Epoch 00016: val_loss improved from 0.52910 to 0.52655, saving model to output/1D_CNN_model_bin_conv64_conv64_filtersize10_relu_bn_maxpool2_globalmaxpool_dropout0.3_dense0_dropout0.2_batch256_lr_0.002/weights.hdf5\n",
      "462/462 [==============================] - 14s 31ms/step - loss: 0.5379 - acc: 0.6886 - auc: 0.7310 - val_loss: 0.5265 - val_acc: 0.6952 - val_auc: 0.7453\n",
      "Epoch 17/100\n",
      "461/462 [============================>.] - ETA: 0s - loss: 0.5373 - acc: 0.6871 - auc: 0.7331\n",
      "Epoch 00017: val_loss did not improve from 0.52655\n",
      "462/462 [==============================] - 14s 30ms/step - loss: 0.5373 - acc: 0.6871 - auc: 0.7332 - val_loss: 0.5742 - val_acc: 0.6906 - val_auc: 0.7424\n",
      "Epoch 18/100\n",
      "461/462 [============================>.] - ETA: 0s - loss: 0.5363 - acc: 0.6877 - auc: 0.7381\n",
      "Epoch 00018: val_loss did not improve from 0.52655\n",
      "462/462 [==============================] - 14s 31ms/step - loss: 0.5363 - acc: 0.6877 - auc: 0.7380 - val_loss: 0.5298 - val_acc: 0.6938 - val_auc: 0.7587\n",
      "Epoch 19/100\n",
      "461/462 [============================>.] - ETA: 0s - loss: 0.5325 - acc: 0.6896 - auc: 0.7437\n",
      "Epoch 00019: val_loss did not improve from 0.52655\n",
      "462/462 [==============================] - 14s 31ms/step - loss: 0.5326 - acc: 0.6895 - auc: 0.7436 - val_loss: 0.5438 - val_acc: 0.6816 - val_auc: 0.7507\n",
      "Epoch 20/100\n",
      "461/462 [============================>.] - ETA: 0s - loss: 0.5320 - acc: 0.6892 - auc: 0.7473\n",
      "Epoch 00020: val_loss improved from 0.52655 to 0.52539, saving model to output/1D_CNN_model_bin_conv64_conv64_filtersize10_relu_bn_maxpool2_globalmaxpool_dropout0.3_dense0_dropout0.2_batch256_lr_0.002/weights.hdf5\n",
      "462/462 [==============================] - 14s 31ms/step - loss: 0.5320 - acc: 0.6893 - auc: 0.7473 - val_loss: 0.5254 - val_acc: 0.6966 - val_auc: 0.7657\n",
      "Epoch 21/100\n",
      "461/462 [============================>.] - ETA: 0s - loss: 0.5282 - acc: 0.6917 - auc: 0.7519\n",
      "Epoch 00021: val_loss did not improve from 0.52539\n",
      "462/462 [==============================] - 14s 31ms/step - loss: 0.5283 - acc: 0.6916 - auc: 0.7519 - val_loss: 0.5611 - val_acc: 0.6910 - val_auc: 0.7555\n",
      "Epoch 22/100\n",
      "461/462 [============================>.] - ETA: 0s - loss: 0.5269 - acc: 0.6937 - auc: 0.7558\n",
      "Epoch 00022: val_loss did not improve from 0.52539\n",
      "462/462 [==============================] - 14s 31ms/step - loss: 0.5268 - acc: 0.6938 - auc: 0.7559 - val_loss: 0.5576 - val_acc: 0.6974 - val_auc: 0.7697\n",
      "Epoch 23/100\n",
      "461/462 [============================>.] - ETA: 0s - loss: 0.5252 - acc: 0.6931 - auc: 0.7586\n",
      "Epoch 00023: val_loss improved from 0.52539 to 0.51660, saving model to output/1D_CNN_model_bin_conv64_conv64_filtersize10_relu_bn_maxpool2_globalmaxpool_dropout0.3_dense0_dropout0.2_batch256_lr_0.002/weights.hdf5\n",
      "462/462 [==============================] - 14s 31ms/step - loss: 0.5252 - acc: 0.6931 - auc: 0.7586 - val_loss: 0.5166 - val_acc: 0.7002 - val_auc: 0.7768\n",
      "Epoch 24/100\n",
      "461/462 [============================>.] - ETA: 0s - loss: 0.5234 - acc: 0.6958 - auc: 0.7620\n",
      "Epoch 00024: val_loss did not improve from 0.51660\n",
      "462/462 [==============================] - 14s 31ms/step - loss: 0.5233 - acc: 0.6960 - auc: 0.7621 - val_loss: 0.5199 - val_acc: 0.7005 - val_auc: 0.7820\n",
      "Epoch 25/100\n",
      "461/462 [============================>.] - ETA: 0s - loss: 0.5214 - acc: 0.6971 - auc: 0.7655\n",
      "Epoch 00025: val_loss improved from 0.51660 to 0.51178, saving model to output/1D_CNN_model_bin_conv64_conv64_filtersize10_relu_bn_maxpool2_globalmaxpool_dropout0.3_dense0_dropout0.2_batch256_lr_0.002/weights.hdf5\n",
      "462/462 [==============================] - 14s 31ms/step - loss: 0.5213 - acc: 0.6971 - auc: 0.7655 - val_loss: 0.5118 - val_acc: 0.7034 - val_auc: 0.7871\n",
      "Epoch 26/100\n",
      "461/462 [============================>.] - ETA: 0s - loss: 0.5184 - acc: 0.6994 - auc: 0.7700\n",
      "Epoch 00026: val_loss did not improve from 0.51178\n",
      "462/462 [==============================] - 14s 31ms/step - loss: 0.5184 - acc: 0.6994 - auc: 0.7700 - val_loss: 0.5189 - val_acc: 0.7095 - val_auc: 0.7842\n",
      "Epoch 27/100\n",
      "461/462 [============================>.] - ETA: 0s - loss: 0.5171 - acc: 0.7021 - auc: 0.7723\n",
      "Epoch 00027: val_loss did not improve from 0.51178\n",
      "462/462 [==============================] - 14s 30ms/step - loss: 0.5171 - acc: 0.7022 - auc: 0.7723 - val_loss: 0.5273 - val_acc: 0.6876 - val_auc: 0.7877\n",
      "Epoch 28/100\n",
      "461/462 [============================>.] - ETA: 0s - loss: 0.5141 - acc: 0.7049 - auc: 0.7767\n",
      "Epoch 00028: val_loss improved from 0.51178 to 0.50792, saving model to output/1D_CNN_model_bin_conv64_conv64_filtersize10_relu_bn_maxpool2_globalmaxpool_dropout0.3_dense0_dropout0.2_batch256_lr_0.002/weights.hdf5\n",
      "462/462 [==============================] - 14s 31ms/step - loss: 0.5141 - acc: 0.7048 - auc: 0.7767 - val_loss: 0.5079 - val_acc: 0.7074 - val_auc: 0.7890\n",
      "Epoch 29/100\n",
      "461/462 [============================>.] - ETA: 0s - loss: 0.5129 - acc: 0.7058 - auc: 0.7794\n",
      "Epoch 00029: val_loss improved from 0.50792 to 0.50613, saving model to output/1D_CNN_model_bin_conv64_conv64_filtersize10_relu_bn_maxpool2_globalmaxpool_dropout0.3_dense0_dropout0.2_batch256_lr_0.002/weights.hdf5\n",
      "462/462 [==============================] - 14s 31ms/step - loss: 0.5129 - acc: 0.7058 - auc: 0.7794 - val_loss: 0.5061 - val_acc: 0.7082 - val_auc: 0.7930\n",
      "Epoch 30/100\n",
      "307/462 [==================>...........] - ETA: 4s - loss: 0.5107 - acc: 0.7063 - auc: 0.7826"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.models import Model, load_model\n",
    "from keras.optimizers import Adam as Adam\n",
    "from keras.layers import Dense, Conv1D, MaxPooling1D, GlobalMaxPool1D, BatchNormalization, Dropout, Activation\n",
    "from keras.layers import GlobalAveragePooling1D, Flatten, SeparableConv1D\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from sklearn.metrics import auc, classification_report, confusion_matrix, accuracy_score, roc_curve, roc_auc_score, f1_score, precision_recall_curve\n",
    "import tensorflow as tf\n",
    "import os, pickle\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "# coefficient of determination (R^2) for regression  (only for Keras tensors)\n",
    "def r_square(y_true, y_pred):\n",
    "    SS_res =  K.sum(K.square(y_true - y_pred)) \n",
    "    SS_tot = K.sum(K.square(y_true - K.mean(y_true))) \n",
    "    return ( 1 - SS_res/(SS_tot + K.epsilon()) )\n",
    "\n",
    "\n",
    "# hyperparamters\n",
    "num_nodes = [64, 64] #, 64, 64, 64]\n",
    "kernel_size = 10\n",
    "pool_size = 2\n",
    "BATCH_SIZE = 512\n",
    "dense_node = 0\n",
    "dropout_rate = 0.3\n",
    "dropout_cnn = 0.2\n",
    "dropout_fc = 0.2\n",
    "learning_rate = 0.002\n",
    "\n",
    "\n",
    "testname = '-'.join([str(num_node) for num_node in num_nodes])\n",
    "print(testname)\n",
    "\n",
    "# 출력 폴더를 생성\n",
    "model_name = 'model_reg_'\n",
    "for num_node in num_nodes:\n",
    "    model_name += '{}_'.format(num_node)\n",
    "model_name += 'filtersize{}_relu_bn_maxpool{}_globalmaxpool_dropout{}_dense{}_dropout{}_batch{}_lr_{}'.format(kernel_size, pool_size, dropout_cnn, dense_node, dropout_fc, BATCH_SIZE, learning_rate)\n",
    "\n",
    "#model_name = 'model_reg_{}_{}_{}_size{}_relu_bn_maxpool{}_globalmaxpool_dense32_dropout{}_batch{}_learning_rate{}'.format(num_nodes[0], num_nodes[1], num_nodes[2], kernel_size, pool_size, dropout_rate, BATCH_SIZE, learning_rate)\n",
    "save_path = \"output/1D_CNN_\"+model_name\n",
    "\n",
    "if not os.path.exists(save_path):\n",
    "    os.mkdir(save_path)\n",
    "weight_path = save_path + \"/weights.hdf5\"\n",
    "\n",
    "\n",
    "# GPU 설정\n",
    "strategy = tf.distribute.MirroredStrategy(devices=[\"/gpu:1\"])\n",
    "with strategy.scope():\n",
    "    \n",
    "    # build a model\n",
    "    model = Sequential()\n",
    "    for num_node in num_nodes:\n",
    "        model.add(Conv1D(filters=num_node, kernel_size=kernel_size, padding='valid', activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling1D(pool_size=pool_size))\n",
    "    #model.add(BatchNormalization())    \n",
    "    model.add(GlobalMaxPool1D())\n",
    "    #model.add(Flatten())\n",
    "    model.add(Dropout(dropout_cnn))\n",
    "    #model.add(Activation('sigmoid'))\n",
    "    #model.add(Dropout(dropout_rate))\n",
    "    if dense_node != 0:\n",
    "        model.add(Dense(dense_node, activation='tanh'))\n",
    "        model.add(Dropout(dropout_fc))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    \n",
    "    # model 학습 설정\n",
    "    model.compile(loss='mse', optimizer=Adam(lr=learning_rate), metrics=[\"mean_absolute_error\"])\n",
    "    hist = model.fit(x_train, y_train/10, validation_split=0.1, epochs=100, batch_size=BATCH_SIZE, #class_weight={0:1, 1:3}, \n",
    "                            callbacks=[ModelCheckpoint(monitor='val_loss', filepath=weight_path, verbose=1, save_best_only=True),\n",
    "                                        EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='auto')])\n",
    "\n",
    "    #tf.keras.backend.clear_session()\n",
    "\n",
    "model.load_weights(weight_path)\n",
    "    \n",
    "# 모델의 아키텍처 및 구조 저장\n",
    "open(save_path + \"/model.json\", \"wt\").write(model.to_json())\n",
    "\n",
    "# 전체 test 샘플을 한번에 예측\n",
    "y_pred = model.predict(x_test).flatten()\n",
    "\n",
    "# 결과를 저장\n",
    "#np.savetxt(save_path+'/pred_y.txt', y_pred)\n",
    "\n",
    "\n",
    "# 모델의 history log 저장 - binary classification\n",
    "for key in hist.history.keys():\n",
    "    if 'auc' in key and not 'val' in key:\n",
    "        auc_key = key\n",
    "#pickle.dump((hist.history['loss'], hist.history['val_loss'], hist.history['accuracy'], hist.history['val_accuracy'], hist.history[auc], hist.history['val_'+auc]), open(save_path+'/history', 'wb'))\n",
    "\n",
    "# 모델의 history log 저장 - regression\n",
    "pickle.dump((hist.history['loss'], hist.history['val_loss'], hist.history['mean_absolute_error'], hist.history['val_mean_absolute_error']), open(save_path+'/history', 'wb'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc, r2_score\n",
    "from numpy import interp\n",
    "from keras import losses, metrics\n",
    "import keras.backend as K\n",
    "\n",
    "### Classification\n",
    "# Model Accuracy of test set\n",
    "#model_y = np.where(y_pred<0.5,0,1)\n",
    "#print('test set accuracy:{:.2f}'.format(np.mean(model_y==y_test_bin)))\n",
    "\n",
    "### Regression\n",
    "# Model MSE of test set\n",
    "#mse_val = K.eval(losses.mean_squared_error(y_test, y_pred))\n",
    "model_err = metrics.RootMeanSquaredError()\n",
    "model_err.update_state(y_test, y_pred)\n",
    "rmse_val = model_err.result().numpy()\n",
    "acc_val = np.mean((y_pred*10>=5)==y_test_bin)\n",
    "print('test set mse:{:.2f}'.format(rmse_val))\n",
    "print('test set accuracy:{:.2f}'.format(acc_val))\n",
    "\n",
    "\n",
    "# Model AUROC\n",
    "false_positive_rate, true_positive_rate, threshold = roc_curve(y_test_bin, y_pred)\n",
    "roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "print('test set auroc:', roc_auc)\n",
    "\n",
    "# Model R_square\n",
    "#print('test set R2:', r2_score(y_test, y_pred*9))\n",
    "\n",
    "\n",
    "# Adding evaluation results to file name\n",
    "# classification\n",
    "#os.rename(save_path, save_path+'_auc{:.4f}_acc{:.4f}'.format(roc_auc,np.mean(model_y==y_test_bin)))\n",
    "\n",
    "# regression\n",
    "os.rename(save_path, \"output/auc{:.4f}_1D_CNN_{}rmse{:.4f}_acc{:.2f}\".format(roc_auc, model_name, rmse_val,acc_val))\n",
    "\n",
    "\n",
    "# plotting roc\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.xlabel(\"False Positive Rate(1 - Specificity)\")\n",
    "plt.ylabel('True Positive Rate(Sensitivity)')\n",
    "\n",
    "plt.plot(false_positive_rate, true_positive_rate, 'b', label='Model 1 (AUC = %0.4f)'% roc_auc)\n",
    "plt.plot([0,1],[1,1],'y--')\n",
    "plt.plot([0,1],[0,1],'r--')\n",
    "\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model history plot\n",
    "- training curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, loss_ax = plt.subplots(figsize=(20,10))\n",
    "\n",
    "#x-axis는 공유하지만 y-axis는 공유x\n",
    "acc_ax = loss_ax.twinx()\n",
    "\n",
    "loss_ax.plot(hist.history['loss'], 'y', linestyle='dashed', label='train loss')\n",
    "loss_ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "\n",
    "acc_ax.plot(hist.history['mean_absolute_error'], 'b', linestyle='dashed',label='train mae')\n",
    "acc_ax.plot(hist.history['val_mean_absolute_error'], 'g', label='val mae')\n",
    "#acc_ax.plot(hist.history[auc_key], 'b', linestyle='dashed',label='train auc')\n",
    "#acc_ax.plot(hist.history['val_'+auc_key], 'g', label='val auc')\n",
    "\n",
    "\n",
    "\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "#acc_ax.set_ylim(0.2,1.0)\n",
    "acc_ax.set_ylabel('accuray')\n",
    "\n",
    "loss_ax.legend(loc='upper left')\n",
    "acc_ax.legend(loc='lower left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
