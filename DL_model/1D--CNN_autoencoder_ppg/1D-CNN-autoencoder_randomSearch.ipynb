{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-22T18:26:39.982764Z",
     "start_time": "2020-12-22T18:26:13.699807Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading train...done\n",
      "x_train shape: (74636, 5000, 2)\n",
      "x_test.shape: (8795, 5000, 2)\n",
      "x_val.shape: (7955, 5000, 2)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "print('loading train...', flush=True, end='')\n",
    "\n",
    "# x를 loading해서 (batch_size, step, channel)\n",
    "input_path = '../dataset/preprocess4/input2/'\n",
    "x_train = np.load(input_path+'x_train.npz', allow_pickle=True)['arr_0']\n",
    "x_test = np.load(input_path+'x_test.npz', allow_pickle=True)['arr_0']\n",
    "x_val = np.load(input_path+'x_val.npz', allow_pickle=True)['arr_0']\n",
    "\n",
    "gender_train = np.load(input_path+'gender_train.npz', allow_pickle=True)['arr_0']\n",
    "gender_test = np.load(input_path+'gender_test.npz', allow_pickle=True)['arr_0']\n",
    "gender_val = np.load(input_path+'gender_val.npz', allow_pickle=True)['arr_0']\n",
    "\n",
    "age_train = np.load(input_path+'age_train.npz', allow_pickle=True)['arr_0']\n",
    "age_test = np.load(input_path+'age_test.npz', allow_pickle=True)['arr_0']\n",
    "age_val = np.load(input_path+'age_val.npz', allow_pickle=True)['arr_0']\n",
    "\n",
    "y_train = np.load(input_path+'y_train.npz')['arr_0']\n",
    "y_test = np.load(input_path+'y_test.npz')['arr_0']\n",
    "y_val = np.load(input_path+'y_val.npz')['arr_0']\n",
    "print('done', flush=True)\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test.shape:', x_test.shape)\n",
    "print('x_val.shape:', x_val.shape)\n",
    "\n",
    "# binary classification\n",
    "y_train_bin = y_train >= 4\n",
    "y_test_bin = y_test >= 4\n",
    "y_val_bin = y_val >= 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_ppg = x_train[:,:,0:1]\n",
    "x_val_ppg = x_val[:,:,0:1]\n",
    "x_test_ppg = x_test[:,:,0:1]\n",
    "\n",
    "x_train_ecg = x_train[:,:,1:2]\n",
    "x_val_ecg = x_val[:,:,1:2]\n",
    "x_test_ecg = x_test[:,:,1:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.models import Model, load_model\n",
    "from keras.optimizers import Adam as Adam\n",
    "from keras.layers import Dense, Conv1D, MaxPooling1D, GlobalMaxPool1D, BatchNormalization, Dropout, Activation\n",
    "from keras.layers import GlobalAveragePooling1D, Flatten, SeparableConv1D, LeakyReLU, UpSampling1D\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from sklearn.metrics import auc, classification_report, confusion_matrix, accuracy_score, roc_curve, roc_auc_score, f1_score, precision_recall_curve\n",
    "import tensorflow as tf\n",
    "import os, pickle\n",
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "from keras.initializers import he_normal, GlorotNormal\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder\n",
    "nfold = 1  # 각각의 hyperparameter에 대해 k-fold 를 시행하고 평균을 구한다.\n",
    "ntest = 100\n",
    "rootdir = \"ECG/pacu/regression/1D_CNN_3layers_train_w_samp3_lowess_filtered\"\n",
    "\n",
    "predirs = []\n",
    "for root, dirs, files in os.walk(rootdir):  # 하위 대상들을 recursive 하게 긁어옴\n",
    "    for filename in dirs:\n",
    "        predirs.append(filename)\n",
    "\n",
    "if not os.path.exists(rootdir):\n",
    "    os.mkdir(rootdir)\n",
    "\n",
    "\n",
    "# test_settings\n",
    "test_settings_1, test_settings_2, test_settings_3, test_settings_4 = [], [], [], []\n",
    "\n",
    "\n",
    "# hyperparamters\n",
    "#num_nodes = [64, 64, 64] #, 64, 64, 64]\n",
    "#kernel_size = 10\n",
    "pool_size = 2\n",
    "\n",
    "#dense_node = 32\n",
    "#dropout_rate = 0.2\n",
    "learning_rate = 0.002\n",
    "\n",
    "# hyperparamters pool\n",
    "num_opts = [32, 64, 128, 256] # num of filters(kernel)\n",
    "stride_opts = [1,1,1,1,1,2,2,2,2,3]\n",
    "kernel_opts = [3,3,3,3,3,5,5,5,5,7] # kernel size\n",
    "dropout_opts  = [0, 0.1, 0.2, 0.3, 0.4, 0.5] # dropout rate\n",
    "dense_opts = [16, 32, 64, 128]\n",
    "globalpool_opts = ['max','ave']\n",
    "BATCH_SIZE = [512, 1024]\n",
    "\n",
    "\n",
    "print('start making test settings...', end='', flush=True)\n",
    "# test settings\n",
    "for num_l1 in num_opts:\n",
    "    for num_l2 in num_opts:\n",
    "        for num_l3 in num_opts:\n",
    "            for num_l4 in num_opts:\n",
    "                for kernel_l1 in kernel_opts:\n",
    "                    for kernel_l2 in kernel_opts:\n",
    "                        for kernel_l3 in kernel_opts:\n",
    "                            for kernel_l4 in kernel_opts:\n",
    "                                test_settings_1.append([num_l1, num_l2, num_l3, num_l4, kernel_l1, kernel_l2, kernel_l3, kernel_l4])\n",
    "\n",
    "for dense1 in dense_opts:\n",
    "    for dropout1 in dropout_opts:\n",
    "        for dropout_fc in dropout_opts:\n",
    "            for globalpool_opt in globalpool_opts:\n",
    "                for batch_size in BATCH_SIZE:\n",
    "                    for conv_double in [True, False]:\n",
    "                        test_settings_2.append([dense1, dropout1, dropout_fc, globalpool_opt, batch_size, conv_double])                                   \n",
    "\n",
    "for stride_l1 in stride_opts:\n",
    "    for stride_l2 in stride_opts:\n",
    "        for stride_l3 in stride_opts:\n",
    "            for stride_l4 in stride_opts:\n",
    "                for stride_l5 in stride_opts:\n",
    "                    for num_l5 in num_opts:\n",
    "                        for kernel_l5 in kernel_opts:\n",
    "                            test_settings_3.append([stride_l1, stride_l2, stride_l3, stride_l4, stride_l5, num_l5, kernel_l5])\n",
    "      \n",
    "for dense2 in dense_opts:\n",
    "    for dropout2 in dropout_opts:\n",
    "        test_settings_4.append([dense2, dropout2])\n",
    "    \n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RandomSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import metrics\n",
    "# random search for hyperparameter\n",
    "ntrial = 200\n",
    "train_errs, val_errs = [] ,[]\n",
    "test_roc, test_prc = [], []\n",
    "test_acc = []\n",
    "random_settings = []\n",
    "\n",
    "\n",
    "for itrial in range(ntrial):\n",
    "    # grid search\n",
    "    # test_setting = test_settings[itrial]\n",
    "\n",
    "    # random search\n",
    "    print('random search {}/{}'.format(itrial, ntrial))\n",
    "    test_setting_1 = random.choice(test_settings_1)\n",
    "    test_setting_2 = random.choice(test_settings_2)\n",
    "    test_setting_3 = random.choice(test_settings_3)\n",
    "    test_setting_4 = random.choice(test_settings_4)    \n",
    "    \n",
    "        \n",
    "    # test_setting\n",
    "    num_l1, num_l2, num_l3, num_l4, kernel_l1, kernel_l2, kernel_l3, kernel_l4 = test_setting_1\n",
    "    dense1, dropout1, dropout_fc, globalpool_opt, batch_size, conv_double = test_setting_2\n",
    "    stride_l1, stride_l2, stride_l3, stride_l4, stride_l5, num_l5, kernel_l5 = test_setting_3\n",
    "    dense2, dropout2 = test_setting_4\n",
    "    \n",
    "    \n",
    "    # total conv layers of the model\n",
    "    n_conv = random.choice([2,3])\n",
    "    \n",
    "    if n_conv==2:\n",
    "        num_l3,kernel_l3,stride_l3 = 0,0,0\n",
    "        num_l4,kernel_l4,stride_l4 = 0,0,0\n",
    "        num_l5,kernel_l5,stride_l5 = 0,0,0\n",
    "    \n",
    "    if n_conv==3:\n",
    "        num_l4,kernel_l4,stride_l4 = 0,0,0\n",
    "        num_l5,kernel_l5,stride_l5 = 0,0,0\n",
    "        \n",
    "    if n_conv==4:\n",
    "        num_l5,kernel_l5,stride_l5 = 0,0,0  \n",
    "        \n",
    "    \n",
    "    n_dense = random.choice([0,1,2])\n",
    "    \n",
    "    if n_dense==0:\n",
    "        dense1, dropout1 = 0,0\n",
    "        dense2, dropout2 = 0,0\n",
    "    \n",
    "    if n_dense==1:\n",
    "        dense2, dropout2 = 0,0\n",
    "        \n",
    "\n",
    "    # 이번 옵션에 대한 결과 디렉토리\n",
    "    odir_f = 'batch={},c1={},c2={},c3={},filt1={},filt2={},filt3={},str1={},str2={},str3={},conv_double={},globalpool={},dropout={},dnodes={},dropout={},dnodes={},dropout={}'.format(batch_size, num_l1, num_l2, num_l3,kernel_l1, kernel_l2, kernel_l3, stride_l1,stride_l2,stride_l3,conv_double, globalpool_opt, dropout_fc, dense1, dropout1, dense2, dropout2)\n",
    "    random_settings.append(odir_f)\n",
    "    \n",
    "    odir = rootdir + '/' + odir_f\n",
    "    if not os.path.exists(odir):\n",
    "        os.mkdir(odir)\n",
    "\n",
    "    weightcache = \"{}/weights.hdf5\".format(odir)        \n",
    "\n",
    "    strategy = tf.distribute.MirroredStrategy(devices=[\"/gpu:0\", \"/gpu:1\"])\n",
    "    with strategy.scope():\n",
    "        # build a model\n",
    "        inp_fnn = Input(shape=(agender_train.shape[1],))\n",
    "        inp_cnn = Input(shape=(x_train.shape[1], x_train.shape[2]))\n",
    "    \n",
    "        out_fnn = inp_fnn\n",
    "        out_fnn = Activation('sigmoid') (out_fnn)\n",
    "        \n",
    "        out_cnn = inp_cnn\n",
    "        act='relu'\n",
    "        \n",
    "        \n",
    "        # c1 layer\n",
    "        if conv_double:\n",
    "            out_cnn = Conv1D(filters=num_l1, kernel_size=kernel_l1, strides=stride_l1, padding='same') (out_cnn)\n",
    "        out_cnn = Conv1D(filters=num_l1, kernel_size=kernel_l1, strides=stride_l1, padding='same') (out_cnn)\n",
    "        out_cnn = Activation('relu') (out_cnn)\n",
    "        out_cnn = BatchNormalization() (out_cnn)\n",
    "        out_cnn = MaxPooling1D(pool_size=pool_size) (out_cnn)\n",
    "\n",
    "\n",
    "        # c2 layer\n",
    "        if conv_double:\n",
    "            out_cnn = Conv1D(filters=num_l2, kernel_size=kernel_l2, strides=stride_l2, padding='same') (out_cnn)\n",
    "        out_cnn = Conv1D(filters=num_l2, kernel_size=kernel_l2, strides=stride_l2, padding='same') (out_cnn)\n",
    "        out_cnn = Activation('relu') (out_cnn)\n",
    "        out_cnn = BatchNormalization() (out_cnn)\n",
    "        out_cnn = MaxPooling1D(pool_size=pool_size) (out_cnn)\n",
    "        \n",
    "        \n",
    "        # c3 layer\n",
    "        if n_conv>2:\n",
    "            if conv_double:\n",
    "                out_cnn = Conv1D(filters=num_l3, kernel_size=kernel_l3, strides=stride_l3, padding='same') (out_cnn)\n",
    "            out_cnn = Conv1D(filters=num_l3, kernel_size=kernel_l3, strides=stride_l3, padding='same') (out_cnn)\n",
    "            out_cnn = Activation('relu') (out_cnn)\n",
    "            out_cnn = BatchNormalization() (out_cnn)\n",
    "            out_cnn = MaxPooling1D(pool_size=pool_size) (out_cnn)\n",
    "        \n",
    "        \n",
    "        # c4 layer\n",
    "        if n_conv>3:\n",
    "            if num_l3 == 512:\n",
    "                model.add(Conv1D(filters=128,kernel_size=1,padding='same'))\n",
    "            if conv_double:\n",
    "                out_cnn = Conv1D(filters=num_l4, kernel_size=kernel_l4, strides=stride_l4, padding='same') (out_cnn)\n",
    "            out_cnn = Conv1D(filters=num_l4, kernel_size=kernel_l4, strides=stride_l4, padding='same') (out_cnn)\n",
    "            out_cnn = Activation('relu') (out_cnn)\n",
    "            out_cnn = BatchNormalization() (out_cnn)\n",
    "            out_cnn = MaxPooling1D(pool_size=pool_size) (out_cnn)\n",
    "            \n",
    "                    \n",
    "\n",
    "\n",
    "        # global이냐 flatten이냐는 따로 모델 나눠야 할듯\n",
    "        if globalpool_opt == 'max':\n",
    "            out_cnn = GlobalMaxPool1D() (out_cnn)\n",
    "        elif globalpool_opt == 'ave':\n",
    "            out_cnn = GlobalAveragePooling1D() (out_cnn)    \n",
    "        out_cnn = Dropout(dropout_fc) (out_cnn)\n",
    "        \n",
    "        \n",
    "        out = concatenate([out_fnn, out_cnn])\n",
    "            \n",
    "        if n_dense>=1:\n",
    "            out = Dense(dense1, activation='tanh') (out)\n",
    "            out = Dropout(dropout1) (out)\n",
    "            \n",
    "        if n_dense==2:\n",
    "            out = Dense(dense2, activation='tanh') (out)\n",
    "            out = Dropout(dropout2) (out)\n",
    "            \n",
    "        out = Dense(1, activation='sigmoid') (out)\n",
    "        \n",
    "        \n",
    "        model = Model(inputs=[inp_cnn, inp_fnn], outputs=[out])\n",
    "\n",
    "\n",
    "        # model 학습 설정\n",
    "        try:\n",
    "            model.compile(loss='binary_crossentropy', optimizer=Adam(lr=learning_rate), metrics=[\"acc\", tf.keras.metrics.AUC()])\n",
    "            hist = model.fit(x_trains, y_train_bin, sample_weight=train_w_samp, validation_data=(x_vals, y_val_bin, val_w_samp), epochs=50, batch_size=batch_size, #class_weight={0:1, 1:3}, \n",
    "                                    callbacks=[ModelCheckpoint(monitor='val_loss', filepath=weightcache, verbose=1, save_best_only=True),\n",
    "                                                EarlyStopping(monitor='val_loss', patience=3, verbose=0, mode='auto')])\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            os.rmdir(odir)\n",
    "            #os.rename(odir,rootdir+'/error_{}'.format(odir_f))\n",
    "            itrial -= 1\n",
    "            test_roc.append(0)\n",
    "            test_acc.append(0)\n",
    "            test_prc.append(0)\n",
    "            train_errs.append(-1)\n",
    "            val_errs.append(-1)\n",
    "            continue\n",
    "            \n",
    "            \n",
    "    # 모델의 아키텍처 및 구조 저장\n",
    "    open(odir+\"/model.json\", \"wt\").write(model.to_json())\n",
    "\n",
    "    # test set에 대한 y_pred 계산\n",
    "    model.load_weights(weightcache)  # fit 함수는 마지막 epoch의 결과를 리턴하기 때문에 best 결과를 다시 읽어들어야함\n",
    "    y_pred = model.predict(x_tests).flatten()\n",
    "\n",
    "    \n",
    "    # acc 계산\n",
    "    acc = metrics.Accuracy()\n",
    "    acc.update_state(y_pred>=0.5, y_test_bin, sample_weight=test_w_samp)\n",
    "    acc_val = acc.result().numpy()\n",
    "    test_acc.append(acc_val)\n",
    "    \n",
    "    # auroc 계산\n",
    "    false_positive_rate, true_positive_rate, threshold = roc_curve(y_test_bin, y_pred, sample_weight=test_w_samp)\n",
    "    roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "    test_roc.append(roc_auc)\n",
    "\n",
    "    # auprc \n",
    "    precision, recall, _ = precision_recall_curve(y_test_bin, y_pred, sample_weight=test_w_samp)\n",
    "    prc_auc = auc(recall, precision)\n",
    "    test_prc.append(prc_auc)\n",
    "\n",
    "    \n",
    "    # rename\n",
    "    os.rename(odir, rootdir+'/roc{:.4f}_prc{:.4f}_{}_acc{:.2f}'.format(roc_auc, prc_auc, odir_f, acc_val))    \n",
    "    \n",
    "    \n",
    "    # train 과정에서의 err\n",
    "    train_err = min(hist.history['loss'])\n",
    "    val_err = min(hist.history['val_loss'])\n",
    "\n",
    "    \n",
    "    val_errs.append(val_err)\n",
    "    train_errs.append(train_err)\n",
    "\n",
    "\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "\n",
    "max_idx = test_roc.index(max(test_auc))\n",
    "print('\\nBest Model roc:{:.4f}, info: {}'.format(test_roc(max_idx), random_settings(max_idx)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
