{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-30T15:52:04.310938Z",
     "start_time": "2020-12-30T15:52:01.994071Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras import losses, metrics\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model, load_model\n",
    "from keras.optimizers import Adam as Adam\n",
    "from keras.layers import Dense, Conv1D, MaxPooling1D, GlobalMaxPool1D, BatchNormalization, Dropout, Activation\n",
    "from keras.layers import GlobalAveragePooling1D, Flatten, SeparableConv1D, Input, LSTM, GRU, concatenate\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from sklearn.metrics import auc, classification_report, confusion_matrix, accuracy_score, roc_curve, roc_auc_score, f1_score, precision_recall_curve\n",
    "import tensorflow as tf\n",
    "import os, sys, pickle\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T16:13:52.020485Z",
     "start_time": "2020-12-18T16:13:47.192369Z"
    }
   },
   "outputs": [],
   "source": [
    "print('loading train...', flush=True, end='')\n",
    "\n",
    "# x를 loading해서 (batch_size, step, channel)\n",
    "input_path = '../dataset/preprocess4/welch/'\n",
    "x_train = np.load(input_path+'x_train.npz', allow_pickle=True)['arr_0']\n",
    "x_test = np.load(input_path+'x_test.npz', allow_pickle=True)['arr_0']\n",
    "x_val = np.load(input_path+'x_val.npz', allow_pickle=True)['arr_0']\n",
    "y_train = np.load(input_path+'y_train.npz')['arr_0']\n",
    "y_test = np.load(input_path+'y_test.npz')['arr_0']\n",
    "y_val = np.load(input_path+'y_val.npz')['arr_0']\n",
    "print('done', flush=True)\n",
    "\n",
    "\n",
    "x_train_ecg = x_train[:,:,:,1]\n",
    "x_val_ecg = x_val[:,:,:,1]\n",
    "x_test_ecg = x_test[:,:,:,1]\n",
    "\n",
    "\n",
    "print('x_train shape:', x_train_ecg.shape)\n",
    "print('x_test.shape:', x_test_ecg.shape)\n",
    "print('x_val.shape:', x_val_ecg.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-30T16:02:45.639884Z",
     "start_time": "2020-12-30T16:00:17.816842Z"
    }
   },
   "outputs": [],
   "source": [
    "print('loading train...', flush=True, end='')\n",
    "\n",
    "# x를 loading해서 (batch_size, step, channel)\n",
    "input_path = '../dataset/preprocess6/welch1/'\n",
    "x_train = np.load(input_path+'x_train_pacu.npz', allow_pickle=True)['arr_0']\n",
    "x_test = np.load(input_path+'x_test_pacu.npz', allow_pickle=True)['arr_0']\n",
    "x_val = np.load(input_path+'x_val_pacu.npz', allow_pickle=True)['arr_0']\n",
    "y_train = np.load(input_path+'y_train_pacu.npz')['arr_0']\n",
    "y_test = np.load(input_path+'y_test_pacu.npz')['arr_0']\n",
    "y_val = np.load(input_path+'y_val_pacu.npz')['arr_0']\n",
    "print('done', flush=True)\n",
    "\n",
    "\n",
    "x_train2 = np.ones(shape=x_train.shape)\n",
    "for idx, x in enumerate(x_train):\n",
    "    x_train2[idx,:,:,0] = pd.DataFrame(x[:,:,0]).fillna(method='ffill', axis=1).fillna(method='bfill', axis=1).values\n",
    "    x_train2[idx,:,:,1] = pd.DataFrame(x[:,:,1]).fillna(method='ffill', axis=1).fillna(method='bfill', axis=1).values\n",
    "\n",
    "\n",
    "x_val2 = np.ones(shape=x_val.shape)\n",
    "for idx, x in enumerate(x_val):\n",
    "    x_val2[idx,:,:,0] = pd.DataFrame(x[:,:,0]).fillna(method='ffill', axis=1).fillna(method='bfill', axis=1).values\n",
    "    x_val2[idx,:,:,1] = pd.DataFrame(x[:,:,1]).fillna(method='ffill', axis=1).fillna(method='bfill', axis=1).values\n",
    "\n",
    "    \n",
    "x_test2 = np.ones(shape=x_test.shape)\n",
    "for idx, x in enumerate(x_test):\n",
    "    x_test2[idx,:,:,0] = pd.DataFrame(x[:,:,0]).fillna(method='ffill', axis=1).fillna(method='bfill', axis=1).values\n",
    "    x_test2[idx,:,:,1] = pd.DataFrame(x[:,:,1]).fillna(method='ffill', axis=1).fillna(method='bfill', axis=1).values\n",
    "    \n",
    "\n",
    "x_train_ecg = x_train2[:,:,:,1]\n",
    "x_val_ecg = x_val2[:,:,:,1]\n",
    "x_test_ecg = x_test2[:,:,:,1]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test.shape:', x_test.shape)\n",
    "print('x_val.shape:', x_val.shape)\n",
    "\n",
    "print('x_train_ppg shape:', x_train_ppg.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-30T15:53:16.094393Z",
     "start_time": "2020-12-30T15:53:16.089287Z"
    }
   },
   "outputs": [],
   "source": [
    "# binary classification\n",
    "y_train_bin = y_train >= 4\n",
    "y_test_bin = y_test >= 4\n",
    "y_val_bin = y_val >= 4\n",
    "y_train_bin2 = y_train >=7\n",
    "y_val_bin2 = y_val>=7\n",
    "y_test_bin2 = y_test>=7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T06:28:28.272145Z",
     "start_time": "2020-12-09T06:28:27.448117Z"
    }
   },
   "outputs": [],
   "source": [
    "### 3 classes\n",
    "# labels for y_train\n",
    "y_train_class = []\n",
    "for i in range(y_train.shape[0]):\n",
    "    if y_train[i] <= 3.5:\n",
    "        y_train_class.append((1,0,0))\n",
    "    elif 3.5<y_train[i]<=6.5:\n",
    "        y_train_class.append((0,1,0))\n",
    "    else:\n",
    "        y_train_class.append((0,0,1))\n",
    "        \n",
    "y_train_class = np.array(y_train_class, int)\n",
    "\n",
    "\n",
    "# labels for y_val\n",
    "y_val_class = []\n",
    "for i in range(y_val.shape[0]):\n",
    "    if y_val[i] <= 3.5:\n",
    "        y_val_class.append((1,0,0))\n",
    "    elif 3.5<y_val[i]<=6.5:\n",
    "        y_val_class.append((0,1,0))\n",
    "    else:\n",
    "        y_val_class.append((0,0,1))\n",
    "        \n",
    "y_val_class = np.array(y_val_class, int)\n",
    "\n",
    "# labels for y_test\n",
    "y_test_class = []\n",
    "for i in range(y_test.shape[0]):\n",
    "    if y_test[i] <= 3.5:\n",
    "        y_test_class.append([1,0,0])\n",
    "    elif 3.5<y_test[i]<=6.5:\n",
    "        y_test_class.append([0,1,0])\n",
    "    else:\n",
    "        y_test_class.append([0,0,1])\n",
    "        \n",
    "y_test_class = np.array(y_test_class, int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 class (NRS>=4, NRS<4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T08:34:04.746253Z",
     "start_time": "2020-12-10T08:34:04.714482Z"
    }
   },
   "outputs": [],
   "source": [
    "# 2 class에 대한 sample weight\n",
    "train_w_samp2 = np.ones(shape=(len(y_train),))\n",
    "train_w_samp2[y_train_bin==0]= len(y_train) / np.sum(y_train_bin)\n",
    "train_w_samp2[y_train_bin!=0]= len(y_train) / np.sum(~y_train_bin)\n",
    "\n",
    "train_w_samp2_2 = np.ones(shape=(len(y_train),))\n",
    "train_w_samp2_2[y_train_bin==0]= len(y_train) / np.sum(y_train_bin2)\n",
    "train_w_samp2_2[y_train_bin!=0]= len(y_train) / np.sum(~y_train_bin2)\n",
    "\n",
    "print('sample weight for no pain: {:.2f}, moderate pain: {:.2f}'\n",
    "      .format(len(y_train) / np.sum(y_train_bin), len(y_train) / np.sum(~y_train_bin)))\n",
    "print('sample weight for no pain: {:.2f}, severe pain: {:.2f}'\n",
    "      .format(len(y_train) / np.sum(y_train_bin2), len(y_train) / np.sum(~y_train_bin2)))\n",
    "\n",
    "\n",
    "# 2 class에 대한 sample weight\n",
    "val_w_samp2 = np.ones(shape=(len(y_val),))\n",
    "val_w_samp2[y_val_bin==0]= len(y_val) / np.sum(y_val_bin)\n",
    "val_w_samp2[y_val_bin!=0]= len(y_val) / np.sum(~y_val_bin)\n",
    "\n",
    "val_w_samp2_2 = np.ones(shape=(len(y_val),))\n",
    "val_w_samp2_2[y_val_bin==0]= len(y_val) / np.sum(y_val_bin2)\n",
    "val_w_samp2_2[y_val_bin!=0]= len(y_val) / np.sum(~y_val_bin2)\n",
    "\n",
    "print('sample weight for no pain: {:.2f}, moderate pain: {:.2f}'\n",
    "      .format(len(y_val) / np.sum(y_val_bin), len(y_val) / np.sum(~y_val_bin)))\n",
    "print('sample weight for no pain: {:.2f}, severe pain: {:.2f}'\n",
    "      .format(len(y_val) / np.sum(y_val_bin2), len(y_val) / np.sum(~y_val_bin2)))\n",
    "\n",
    "\n",
    "# 2 class에 대한 sample weight\n",
    "test_w_samp2 = np.ones(shape=(len(y_test),))\n",
    "test_w_samp2[y_test_bin==0]= len(y_test) / np.sum(y_test_bin)\n",
    "test_w_samp2[y_test_bin!=0]= len(y_test) / np.sum(~y_test_bin)\n",
    "\n",
    "test_w_samp2_2 = np.ones(shape=(len(y_test),))\n",
    "test_w_samp2_2[y_test_bin==0]= len(y_test) / np.sum(y_test_bin2)\n",
    "test_w_samp2_2[y_test_bin!=0]= len(y_test) / np.sum(~y_test_bin2)\n",
    "\n",
    "print('sample weight for no pain: {:.2f}, moderate pain: {:.2f}'\n",
    "      .format(len(y_test) / np.sum(y_test_bin), len(y_test) / np.sum(~y_test_bin)))\n",
    "print('sample weight for no pain: {:.2f}, severe pain: {:.2f}'\n",
    "      .format(len(y_test) / np.sum(y_test_bin2), len(y_test) / np.sum(~y_test_bin2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 class (NRS>=7, 7>NRS>=4, NRS<4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T16:14:39.056865Z",
     "start_time": "2020-12-18T16:14:39.032999Z"
    }
   },
   "outputs": [],
   "source": [
    "# 3 class에 대한 sample weight\n",
    "train_w_samp3 = np.ones(shape=(len(y_train),))\n",
    "\n",
    "train_w_samp3[y_train<4]= len(y_train)/np.sum(y_train<4)\n",
    "train_w_samp3[(y_train>=4)&(y_train<7)]= len(y_train)/np.sum((4<=y_train)&(y_train<7))\n",
    "train_w_samp3[y_train>=7]= len(y_train)/np.sum(y_train>=7)\n",
    "\n",
    "print('train set')\n",
    "print('sample weight for class 1: {:.2f}, class 2: {:.2f}, class 3: {:.2f}\\n'\n",
    "      .format(len(y_train)/np.sum(y_train<4),len(y_train)/np.sum((y_train>=4)&(y_train<7)),len(y_train)/np.sum(y_train>=7)))\n",
    "\n",
    "# 3 class에 대한 sample weight\n",
    "val_w_samp3 = np.ones(shape=(len(y_val),))\n",
    "\n",
    "val_w_samp3[y_val<4]= len(y_val)/np.sum(y_val<4)\n",
    "val_w_samp3[(y_val>=4)&(y_val<7)]= len(y_val)/np.sum((4<=y_val)&(y_val<7))\n",
    "val_w_samp3[y_val>=7]= len(y_val)/np.sum(y_val>=7)\n",
    "\n",
    "print('val set')\n",
    "print('sample weight for class 1: {:.2f}, class 2: {:.2f}, class 3: {:.2f}\\n'\n",
    "      .format(len(y_val)/np.sum(y_val<4),len(y_val)/np.sum((y_val>=4)&(y_val<7)),len(y_val)/np.sum(y_val>=7)))\n",
    "\n",
    "# 3 class에 대한 sample weight\n",
    "test_w_samp3 = np.ones(shape=(len(y_test),))\n",
    "\n",
    "test_w_samp3[y_test<4]= len(y_test)/np.sum(y_test<4)\n",
    "test_w_samp3[(y_test>=4)&(y_test<7)]= len(y_test)/np.sum((4<=y_test)&(y_test<7))\n",
    "test_w_samp3[y_test>=7]= len(y_test)/np.sum(y_test>=7)\n",
    "\n",
    "print('test set')\n",
    "print('sample weight for class 1: {:.2f}, class 2: {:.2f}, class 3: {:.2f}'\n",
    "      .format(len(y_test)/np.sum(y_test<4),len(y_test)/np.sum((y_test>=4)&(y_test<7)),len(y_test)/np.sum(y_test>=7)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-30T16:02:50.482326Z",
     "start_time": "2020-12-30T16:02:50.470771Z"
    }
   },
   "outputs": [],
   "source": [
    "# 3 class에 대한 sample weight\n",
    "train_w_samp3 = np.ones(shape=(len(y_train),))\n",
    "\n",
    "train_w_samp3[y_train<4]= len(y_train)/np.sum(y_train<4)\n",
    "train_w_samp3[(y_train>=4)&(y_train<7)]= len(y_train)/np.sum((4<=y_train)&(y_train<7))\n",
    "train_w_samp3[y_train>=7]= len(y_train)/np.sum(y_train>=7)\n",
    "\n",
    "\n",
    "print('train set')\n",
    "print('sample weight for class 1: {:.2f}, class 2: {:.2f}, class 3: {:.2f}\\n'\n",
    "      .format(len(y_train)/np.sum(y_train<4),len(y_train)/np.sum((y_train>=4)&(y_train<7)),len(y_train)/np.sum(y_train>=7)))\n",
    "\n",
    "\n",
    "val_w_samp3 = np.ones(len(y_val))\n",
    "\n",
    "test_w_samp3 = np.ones(shape=(len(y_test),))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-30T15:52:30.071564Z",
     "start_time": "2020-12-30T15:52:27.063712Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-30T16:02:58.494620Z",
     "start_time": "2020-12-30T16:02:55.275395Z"
    }
   },
   "outputs": [],
   "source": [
    "# folder\n",
    "nfold = 1  # 각각의 hyperparameter에 대해 k-fold 를 시행하고 평균을 구한다.\n",
    "ntest = 100\n",
    "rootdir = \"preprocess6/NRS>=4_welch1/LSTM_3layers_sample-weighted_3class\"\n",
    "\n",
    "predirs = []\n",
    "for root, dirs, files in os.walk(rootdir):  # 하위 대상들을 recursive 하게 긁어옴\n",
    "    for filename in dirs:\n",
    "        predirs.append(filename)\n",
    "\n",
    "if not os.path.exists(rootdir):\n",
    "    os.mkdir(rootdir)\n",
    "\n",
    "\n",
    "# test_settings\n",
    "test_settings_1, test_settings_2, test_settings_3 = [], [], []\n",
    "\n",
    "\n",
    "# hyperparamters\n",
    "#num_nodes = [64, 64, 64] #, 64, 64, 64]\n",
    "#kernel_size = 10\n",
    "pool_size = 2\n",
    "\n",
    "#dense_node = 32\n",
    "#dropout_rate = 0.2\n",
    "learning_rate = 0.002\n",
    "\n",
    "# hyperparamters pool\n",
    "dropout_opts  = [0, 0.1, 0.2, 0.3, 0.4, 0.5] # dropout rate\n",
    "dense_opts = [8, 16, 32, 64]\n",
    "BATCH_SIZE = [512, 1024]\n",
    "\n",
    "\n",
    "unit_opts = [4, 8, 16, 32, 64, 128, 256]\n",
    "\n",
    "print('start making test settings...', end='', flush=True)\n",
    "# test settings\n",
    "for unit1 in unit_opts:\n",
    "    for unit2 in unit_opts:\n",
    "        for unit3 in unit_opts:\n",
    "            for dense_node1 in dense_opts:\n",
    "                for dense_node2 in dense_opts:\n",
    "                    for dropout1 in dropout_opts:\n",
    "                        for dropout2 in dropout_opts:\n",
    "                            for dropout3 in dropout_opts:\n",
    "                                for batch_size in BATCH_SIZE:\n",
    "                                    test_settings_1.append([unit1, unit2, unit3, dense_node1, dense_node2, dropout1, dropout2, dropout3, batch_size])\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RandomSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-30T16:03:56.700841Z",
     "start_time": "2020-12-30T16:03:03.255147Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras import metrics\n",
    "# random search for hyperparameter\n",
    "ntrial = 500\n",
    "train_errs, val_errs = [] ,[]\n",
    "test_roc, test_prc = [], []\n",
    "test_acc = []\n",
    "random_settings = []\n",
    "\n",
    "\n",
    "for itrial in range(ntrial):\n",
    "    # grid search\n",
    "    # test_setting = test_settings[itrial]\n",
    "\n",
    "    # random search\n",
    "    print('random search {}/{}'.format(itrial, ntrial))\n",
    "    test_setting_1 = random.choice(test_settings_1)\n",
    "    \n",
    "    rnn_type = random.choice(['lstm','gru'])\n",
    "        \n",
    "    # test_setting\n",
    "    unit1, unit2, unit3, dense_node1, dense_node2, dropout1, dropout2, dropout3, batch_size = test_setting_1\n",
    "    \n",
    "    \n",
    "    # total LSTM layers of the model\n",
    "    n_unit = random.choice([1,2,3])   \n",
    "\n",
    "    if n_unit==1:\n",
    "        units = [unit1]\n",
    "\n",
    "    if n_unit==2:\n",
    "        units = [unit1, unit2]\n",
    "        \n",
    "    if n_unit==3:\n",
    "        units = [unit1, unit2, unit3]\n",
    "        \n",
    "        \n",
    "    # final dense layers\n",
    "    n_dense = random.choice([0,1])\n",
    "    \n",
    "    if n_dense ==0:\n",
    "        dense_node1 = 0\n",
    "        dropout2 = 0 \n",
    "        dense_node2 = 0\n",
    "        dropout3 = 0\n",
    "    \n",
    "    if n_dense ==1:\n",
    "        dense_node2 = 0\n",
    "        dropout3 = 0\n",
    "        \n",
    "    \n",
    "    \n",
    "    # 이번 옵션에 대한 결과 디렉토리\n",
    "    odir_f = '{}_batch={},'.format(rnn_type, batch_size)    \n",
    "    for unit in units:\n",
    "        odir_f += 'unit{},'.format(unit)\n",
    "    odir_f += 'dropout={},dnodes={},dropout={},dnodes={},dropout={}'.format(dropout1, dense_node1, dropout2, dense_node2, dropout3)\n",
    "    random_settings.append(odir_f)\n",
    "    \n",
    "    odir = rootdir + '/' + odir_f\n",
    "    if not os.path.exists(odir):\n",
    "        os.mkdir(odir)\n",
    "\n",
    "    weightcache = \"{}/weights.hdf5\".format(odir)        \n",
    "\n",
    "    strategy = tf.distribute.MirroredStrategy(devices=[\"/gpu:1\"])\n",
    "    with strategy.scope():\n",
    "        \n",
    "        # build a model - function api\n",
    "        inp = Input(shape=(x_train.shape[1], x_train.shape[2]))\n",
    "\n",
    "        if rnn_type == 'lstm':\n",
    "            # lstm layer\n",
    "            inp_lstm = inp\n",
    "            for unit in units[:-1]:\n",
    "                inp_lstm = LSTM(unit, return_sequences=True) (inp_lstm)\n",
    "            out_lstm = LSTM(units[-1]) (inp_lstm)\n",
    "\n",
    "        else:\n",
    "            # gru layer\n",
    "            inp_lstm = inp\n",
    "            for unit in units[:-1]:\n",
    "                inp_lstm = GRU(unit, return_sequences=True) (inp_lstm)\n",
    "            out_lstm = GRU(units[-1]) (inp_lstm)            \n",
    "            \n",
    "            \n",
    "        out_lstm = Dropout(dropout1) (out_lstm)\n",
    "\n",
    "        if n_dense >= 1:\n",
    "            out_lstm = Dense(dense_node1, activation='tanh') (out_lstm)\n",
    "            out_lstm = Dropout(dropout2) (out_lstm)\n",
    "            \n",
    "        if n_dense >= 2:\n",
    "            out_lstm = Dense(dense_node2, activation='tanh') (out_lstm)\n",
    "            out_lstm = Dropout(dropout3) (out_lstm)            \n",
    "            \n",
    "\n",
    "        # output\n",
    "        out = Dense(1, activation='sigmoid') (out_lstm)\n",
    "        model = Model(inputs=[inp], outputs=[out])\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        # model 학습 설정\n",
    "        try:\n",
    "            model.compile(loss='binary_crossentropy', optimizer=Adam(lr=learning_rate), metrics=[\"acc\", tf.keras.metrics.AUC()])\n",
    "            hist = model.fit(x_train_ecg, y_train_bin, sample_weight=train_w_samp3, validation_data=(x_val_ecg, y_val_bin, val_w_samp3), epochs=100, batch_size=batch_size, #class_weight={0:1, 1:3}, \n",
    "                                    callbacks=[ModelCheckpoint(monitor='val_loss', filepath=weightcache, verbose=1, save_best_only=True),\n",
    "                                                EarlyStopping(monitor='val_loss', patience=5, verbose=0, mode='auto')])\n",
    "        except:\n",
    "            os.rmdir(odir)\n",
    "            #os.rename(odir,rootdir+'/error_{}'.format(odir_f))\n",
    "            itrial -= 1\n",
    "            test_roc.append(0)\n",
    "            test_acc.append(0)\n",
    "            test_prc.append(0)\n",
    "            train_errs.append(-1)\n",
    "            val_errs.append(-1)\n",
    "            continue\n",
    "            \n",
    "            \n",
    "    # 모델의 아키텍처 및 구조 저장\n",
    "    open(odir+\"/model.json\", \"wt\").write(model.to_json())\n",
    "\n",
    "    # test set에 대한 y_pred 계산\n",
    "    model.load_weights(weightcache)  # fit 함수는 마지막 epoch의 결과를 리턴하기 때문에 best 결과를 다시 읽어들어야함\n",
    "    y_pred = model.predict(x_test_ecg).flatten()\n",
    "\n",
    "    \n",
    "    # acc 계산\n",
    "    acc = metrics.Accuracy()\n",
    "    acc.update_state(y_pred>=0.5, y_test_bin, sample_weight=test_w_samp3)\n",
    "    acc_val = acc.result().numpy()\n",
    "    test_acc.append(acc_val)\n",
    "    \n",
    "    # auroc 계산\n",
    "    false_positive_rate, true_positive_rate, threshold = roc_curve(y_test_bin, y_pred, sample_weight=test_w_samp3)\n",
    "    roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "    test_roc.append(roc_auc)\n",
    "\n",
    "    # auprc \n",
    "    precision, recall, _ = precision_recall_curve(y_test_bin, y_pred, sample_weight=test_w_samp3)\n",
    "    prc_auc = auc(recall, precision)\n",
    "    test_prc.append(prc_auc)\n",
    "\n",
    "    \n",
    "    # rename\n",
    "    os.rename(odir, rootdir+'/roc{:.4f}_prc{:.4f}_{}_acc{:.2f}'.format(roc_auc, prc_auc, odir_f, acc_val))\n",
    "\n",
    "    # train 과정에서의 err\n",
    "    train_err = min(hist.history['loss'])\n",
    "    val_err = min(hist.history['val_loss'])\n",
    "\n",
    "    val_errs.append(val_err)\n",
    "    train_errs.append(train_err)\n",
    "\n",
    "\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "\n",
    "max_idx = test_roc.index(max(test_auc))\n",
    "print('\\nBest Model roc:{:.4f}, info: {}'.format(test_roc(max_idx), random_settings(max_idx)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## age+gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T16:15:30.876941Z",
     "start_time": "2020-12-18T16:15:30.843649Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "print('loading train...', flush=True, end='')\n",
    "\n",
    "# x를 loading해서 (batch_size, step, channel)\n",
    "input_path = '../dataset/preprocess4/input2/'\n",
    "\n",
    "gender_train = np.load(input_path+'gender_train.npz', allow_pickle=True)['arr_0']\n",
    "gender_test = np.load(input_path+'gender_test.npz', allow_pickle=True)['arr_0']\n",
    "gender_val = np.load(input_path+'gender_val.npz', allow_pickle=True)['arr_0']\n",
    "\n",
    "age_train = np.load(input_path+'age_train.npz', allow_pickle=True)['arr_0']\n",
    "age_test = np.load(input_path+'age_test.npz', allow_pickle=True)['arr_0']\n",
    "age_val = np.load(input_path+'age_val.npz', allow_pickle=True)['arr_0']\n",
    "print('done', flush=True)\n",
    "\n",
    "\n",
    "input_path = '../dataset/preprocess4/welch/'\n",
    "train_mask = pickle.load(open(input_path+'train_mask_pacu', 'rb')) + pickle.load(open(input_path+'train_mask_preop', 'rb'))\n",
    "val_mask = pickle.load(open(input_path+'val_mask_pacu', 'rb')) + pickle.load(open(input_path+'val_mask_preop', 'rb'))\n",
    "test_mask = pickle.load(open(input_path+'test_mask_pacu', 'rb')) + pickle.load(open(input_path+'test_mask_preop', 'rb'))\n",
    "\n",
    "print(len(train_mask), np.sum(train_mask), len(x_train_ecg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T16:16:27.186954Z",
     "start_time": "2020-12-18T16:16:27.170713Z"
    }
   },
   "outputs": [],
   "source": [
    "len(x_train_ecg), len(gender_train), (np.sum(train_mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T16:17:41.203360Z",
     "start_time": "2020-12-18T16:17:40.982385Z"
    }
   },
   "outputs": [],
   "source": [
    "# input, sample weight settings\n",
    "train_w_samp = train_w_samp3\n",
    "val_w_samp = val_w_samp3\n",
    "test_w_samp = test_w_samp3\n",
    "\n",
    "# \n",
    "gender_train = gender_train[train_mask]\n",
    "gender_val = gender_val[val_mask]\n",
    "gender_test = gender_test[test_mask]\n",
    "\n",
    "age_train = age_train[train_mask]\n",
    "age_val = age_val[val_mask]\n",
    "age_test = age_test[test_mask]\n",
    "\n",
    "\n",
    "agender_train = np.array([[age_train[i], gender_train[i]] for i in range(len(age_train))])\n",
    "agender_val = np.array([[age_val[i], gender_val[i]] for i in range(len(age_val))])\n",
    "agender_test = np.array([[age_test[i], gender_test[i]] for i in range(len(age_test))])\n",
    "\n",
    "x_trains = [x_train_ecg, agender_train]\n",
    "x_vals = [x_val_ecg, agender_val]\n",
    "x_tests = [x_test_ecg, agender_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-19T03:16:31.473514Z",
     "start_time": "2020-12-18T16:18:04.262431Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras import metrics\n",
    "# random search for hyperparameter\n",
    "ntrial = 700\n",
    "train_errs, val_errs = [] ,[]\n",
    "test_roc, test_prc = [], []\n",
    "test_acc = []\n",
    "random_settings = []\n",
    "\n",
    "\n",
    "for itrial in range(ntrial):\n",
    "    # grid search\n",
    "    # test_setting = test_settings[itrial]\n",
    "\n",
    "    # random search\n",
    "    print('random search {}/{}'.format(itrial, ntrial))\n",
    "    test_setting_1 = random.choice(test_settings_1)\n",
    "    \n",
    "    rnn_type = random.choice(['lstm','gru'])\n",
    "        \n",
    "    # test_setting\n",
    "    unit1, unit2, unit3, dense_node1, dense_node2, dropout1, dropout2, dropout3, batch_size = test_setting_1\n",
    "    \n",
    "    \n",
    "    # total LSTM layers of the model\n",
    "    n_unit = random.choice([1,2,3])   \n",
    "\n",
    "    if n_unit==1:\n",
    "        units = [unit1]\n",
    "\n",
    "    if n_unit==2:\n",
    "        units = [unit1, unit2]\n",
    "        \n",
    "    if n_unit==3:\n",
    "        units = [unit1, unit2, unit3]\n",
    "        \n",
    "        \n",
    "    # final dense layers\n",
    "    n_dense = random.choice([0,1])\n",
    "    \n",
    "    if n_dense ==0:\n",
    "        dense_node1 = 0\n",
    "        dropout2 = 0 \n",
    "        dense_node2 = 0\n",
    "        dropout3 = 0\n",
    "    \n",
    "    if n_dense ==1:\n",
    "        dense_node2 = 0\n",
    "        dropout3 = 0\n",
    "        \n",
    "    \n",
    "    \n",
    "    # 이번 옵션에 대한 결과 디렉토리\n",
    "    odir_f = '{}_batch={},'.format(rnn_type, batch_size)    \n",
    "    for unit in units:\n",
    "        odir_f += 'unit{},'.format(unit)\n",
    "    odir_f += 'dropout={},dnodes={},dropout={}'.format(dropout1, dense_node1, dropout2)\n",
    "    random_settings.append(odir_f)\n",
    "    \n",
    "    odir = rootdir + '/' + odir_f\n",
    "    if not os.path.exists(odir):\n",
    "        os.mkdir(odir)\n",
    "\n",
    "    weightcache = \"{}/weights.hdf5\".format(odir)        \n",
    "\n",
    "    strategy = tf.distribute.MirroredStrategy(devices=[\"/gpu:1\"])\n",
    "    with strategy.scope():\n",
    "        \n",
    "        # build a model - function api\n",
    "        inp_fnn = Input(shape=(agender_train.shape[1],))\n",
    "        inp = Input(shape=(x_train_ecg.shape[1], x_train_ecg.shape[2]))\n",
    "        inp_lstm = inp\n",
    "        \n",
    "        out_fnn = inp_fnn\n",
    "        out_fnn = Activation('sigmoid') (out_fnn)\n",
    "        \n",
    "\n",
    "        if rnn_type == 'lstm':\n",
    "            # lstm layer\n",
    "            for unit in units[:-1]:\n",
    "                inp_lstm = LSTM(unit, return_sequences=True) (inp_lstm)\n",
    "            out_lstm = LSTM(units[-1]) (inp_lstm)\n",
    "\n",
    "        else:\n",
    "            # gru layer\n",
    "            for unit in units[:-1]:\n",
    "                inp_lstm = GRU(unit, return_sequences=True) (inp_lstm)\n",
    "            out_lstm = GRU(units[-1]) (inp_lstm)            \n",
    "            \n",
    "            \n",
    "        out_lstm = Dropout(dropout1) (out_lstm)\n",
    "        out = concatenate([out_fnn, out_lstm])\n",
    "\n",
    "        if n_dense >= 1:\n",
    "            out = Dense(dense_node1, activation='tanh') (out)\n",
    "            out = Dropout(dropout2) (out)         \n",
    "            \n",
    "\n",
    "        # output\n",
    "        out = Dense(1, activation='sigmoid') (out)\n",
    "        model = Model(inputs=[inp, inp_fnn], outputs=[out])\n",
    "\n",
    "\n",
    "        # model 학습 설정\n",
    "        try:\n",
    "            model.compile(loss='binary_crossentropy', optimizer=Adam(lr=learning_rate), metrics=[\"acc\", tf.keras.metrics.AUC()])\n",
    "            hist = model.fit(x_trains, y_train_bin, sample_weight=train_w_samp3, validation_data=(x_vals, y_val_bin, val_w_samp3), epochs=100, batch_size=batch_size, #class_weight={0:1, 1:3}, \n",
    "                                    callbacks=[ModelCheckpoint(monitor='val_loss', filepath=weightcache, verbose=1, save_best_only=True),\n",
    "                                                EarlyStopping(monitor='val_loss', patience=3, verbose=0, mode='auto')])\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            os.rmdir(odir)\n",
    "            #os.rename(odir,rootdir+'/error_{}'.format(odir_f))\n",
    "            itrial -= 1\n",
    "            test_roc.append(0)\n",
    "            test_acc.append(0)\n",
    "            test_prc.append(0)\n",
    "            train_errs.append(-1)\n",
    "            val_errs.append(-1)\n",
    "            continue\n",
    "            \n",
    "            \n",
    "    # 모델의 아키텍처 및 구조 저장\n",
    "    open(odir+\"/model.json\", \"wt\").write(model.to_json())\n",
    "\n",
    "    # test set에 대한 y_pred 계산\n",
    "    model.load_weights(weightcache)  # fit 함수는 마지막 epoch의 결과를 리턴하기 때문에 best 결과를 다시 읽어들어야함\n",
    "    y_pred = model.predict(x_tests).flatten()\n",
    "\n",
    "    \n",
    "    # acc 계산\n",
    "    acc = metrics.Accuracy()\n",
    "    acc.update_state(y_pred>=0.5, y_test_bin, sample_weight=test_w_samp3)\n",
    "    acc_val = acc.result().numpy()\n",
    "    test_acc.append(acc_val)\n",
    "    \n",
    "    # auroc 계산\n",
    "    false_positive_rate, true_positive_rate, threshold = roc_curve(y_test_bin, y_pred, sample_weight=test_w_samp3)\n",
    "    roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "    test_roc.append(roc_auc)\n",
    "\n",
    "    # auprc \n",
    "    precision, recall, _ = precision_recall_curve(y_test_bin, y_pred, sample_weight=test_w_samp3)\n",
    "    prc_auc = auc(recall, precision)\n",
    "    test_prc.append(prc_auc)\n",
    "\n",
    "    \n",
    "    # rename\n",
    "    os.rename(odir, rootdir+'/roc{:.4f}_prc{:.4f}_{}_acc{:.2f}'.format(roc_auc, prc_auc, odir_f, acc_val))\n",
    "\n",
    "    # train 과정에서의 err\n",
    "    train_err = min(hist.history['loss'])\n",
    "    val_err = min(hist.history['val_loss'])\n",
    "\n",
    "    val_errs.append(val_err)\n",
    "    train_errs.append(train_err)\n",
    "\n",
    "\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "\n",
    "max_idx = test_roc.index(max(test_auc))\n",
    "print('\\nBest Model roc:{:.4f}, info: {}'.format(test_roc(max_idx), random_settings(max_idx)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-05T23:27:38.350Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras import metrics\n",
    "from keras.layers import LeakyReLU, ReLU\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve\n",
    "\n",
    "# random search for hyperparameter\n",
    "ntrial = 200\n",
    "train_errs, val_errs = [] ,[]\n",
    "test_roc, test_prc = [], []\n",
    "test_rmse, test_acc = [], []\n",
    "random_settings = []\n",
    "\n",
    "\n",
    "for itrial in range(ntrial):\n",
    "    # grid search\n",
    "    # test_setting = test_settings[itrial]\n",
    "\n",
    "    # random search\n",
    "    print('random search {}/{}'.format(itrial, ntrial))\n",
    "    test_setting_1 = random.choice(test_settings_1)\n",
    "    test_setting_2 = random.choice(test_settings_2)\n",
    "    \n",
    "\n",
    "    # test_setting\n",
    "    num_l1, num_l2, num_l3, num_l4, kernel_l1, kernel_l2, kernel_l3, kernel_l4 = test_setting_1\n",
    "    dense_node, dropout_cnn, dropout_fc, globalpool_opt, batch_size, conv_double = test_setting_2\n",
    "\n",
    "        \n",
    "\n",
    "    # 이번 옵션에 대한 결과 디렉토리\n",
    "    odir_f = 'batch={}, c1={}, c2={}, c3={}, c4={}, c1filts={}, c2filts={}, c3filts={}, c4filts={}, conv_double={}, globalpool_opt={}, dropout={}, dnodes={}, dropout={}'.format(batch_size, num_l1, num_l2, num_l3, num_l4, kernel_l1, kernel_l2, kernel_l3, kernel_l4, conv_double, globalpool_opt, dropout_cnn, dense_node, dropout_fc)\n",
    "    random_settings.append(odir_f)\n",
    "    \n",
    "    odir = rootdir + '/' + odir_f\n",
    "    if not os.path.exists(odir):\n",
    "        os.mkdir(odir)\n",
    "\n",
    "    weightcache = \"{}/model.hdf5\".format(odir)        \n",
    "\n",
    "    strategy = tf.distribute.MirroredStrategy(devices=[\"/gpu:2\", \"/gpu:3\"])\n",
    "    with strategy.scope():\n",
    "        # build a model\n",
    "        model = Sequential()\n",
    "\n",
    "        conv_act = True\n",
    "        if conv_act:\n",
    "            act = 'relu'\n",
    "        else:\n",
    "            act = None\n",
    "\n",
    "        # c1 layer\n",
    "        if conv_double:\n",
    "            model.add(Conv1D(filters=num_l1, kernel_size=kernel_l1, padding='same'))\n",
    "        model.add(Conv1D(filters=num_l1, kernel_size=kernel_l1, padding='same', activation=act))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling1D(pool_size=pool_size))\n",
    "\n",
    "        # c2 layer\n",
    "        if conv_double:\n",
    "            model.add(Conv1D(filters=num_l2, kernel_size=kernel_l2, padding='same'))\n",
    "        model.add(Conv1D(filters=num_l2, kernel_size=kernel_l2, padding='same', activation=act))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling1D(pool_size=pool_size))\n",
    "        \n",
    "        \n",
    "        # c3 layer\n",
    "        if conv_double:\n",
    "            model.add(Conv1D(filters=num_l3, kernel_size=kernel_l3, padding='same'))\n",
    "        model.add(Conv1D(filters=num_l3, kernel_size=kernel_l3, padding='same', activation=act))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling1D(pool_size=pool_size))\n",
    "        \n",
    "        \n",
    "        # c4 layer\n",
    "        if conv_double:\n",
    "            model.add(Conv1D(filters=num_l4, kernel_size=kernel_l4, padding='same'))\n",
    "        model.add(Conv1D(filters=num_l4, kernel_size=kernel_l4, padding='same', activation=act))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling1D(pool_size=pool_size))\n",
    "\n",
    "\n",
    "        # global이냐 flatten이냐는 따로 모델 나눠야 할듯\n",
    "        if globalpool_opt == 'max':\n",
    "            model.add(GlobalMaxPool1D())\n",
    "        elif globalpool_opt == 'ave':\n",
    "            model.add(GlobalAveragePooling1D())\n",
    "            \n",
    "            \n",
    "        if dense_node != 0:\n",
    "            model.add(Dropout(dropout_cnn))\n",
    "            model.add(Dense(dense_node, activation='tanh'))\n",
    "        model.add(Dropout(dropout_fc))\n",
    "        model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "        try:\n",
    "            # model 학습 설정\n",
    "            model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=learning_rate), metrics=[\"acc\", tf.keras.metrics.AUC()])\n",
    "            hist = model.fit(x_train, y_train_class, validation_data=(x_val, y_val_class), epochs=100, batch_size=batch_size, #class_weight={0:1, 1:3}, \n",
    "                                    callbacks=[ModelCheckpoint(monitor='val_loss', filepath=weightcache, verbose=1, save_best_only=True),\n",
    "                                                EarlyStopping(monitor='val_loss', patience=5, verbose=0, mode='auto')])\n",
    "        except:\n",
    "            os.rename(odir,rootdir+'/error_{}'.format(odir_f))\n",
    "            test_roc.append(0)\n",
    "            test_acc.append(0)\n",
    "            test_prc.append(0)\n",
    "            train_errs.append(-1)\n",
    "            val_errs.append(-1)\n",
    "            continue\n",
    "\n",
    "    # 모델의 아키텍처 및 구조 저장\n",
    "    open(odir+\"/model.json\", \"wt\").write(model.to_json())\n",
    "\n",
    "    # test set에 대한 y_pred 계산\n",
    "    model.load_weights(weightcache)  # fit 함수는 마지막 epoch의 결과를 리턴하기 때문에 best 결과를 다시 읽어들어야함\n",
    "    y_pred = model.predict(x_test)\n",
    "\n",
    "    # auroc 계산\n",
    "    false_positive_rate, true_positive_rate, threshold = roc_curve((y_test_class[:,1]+y_test_class[:,2])>=1, y_pred[:,1]+y_pred[:,2])\n",
    "    roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "    test_roc.append(roc_auc)\n",
    "    \n",
    "    # auprc \n",
    "    precision, recall, _ = precision_recall_curve((y_test_class[:,1]+y_test_class[:,2])>=1, y_pred[:,1]+y_pred[:,2])\n",
    "    prc_auc = auc(recall, precision)\n",
    "    test_prc.append(prc_auc)\n",
    "    \n",
    "    # acc 계산\n",
    "    l_test = np.argmax(y_test_class, axis=1)\n",
    "    l_pred = np.argmax(y_pred, axis=1)\n",
    "    acc_val = accuracy_score(l_test, l_pred)\n",
    "    test_acc.append(acc_val)\n",
    "    \n",
    "    # rename\n",
    "    os.rename(odir, rootdir+'/auc{:.4f}_prc{:.4f}_{}_acc{:.2f}'.format(roc_auc, prc_auc, odir_f, acc_val))\n",
    "\n",
    "    # train 과정에서의 err\n",
    "    train_err = min(hist.history['loss'])\n",
    "    val_err = min(hist.history['val_loss'])\n",
    "\n",
    "    val_errs.append(val_err)\n",
    "    train_errs.append(train_err)\n",
    "\n",
    "\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "\n",
    "max_idx = test_auc.index(max(test_auc))\n",
    "print('\\nBest Model roc:{:.4f}, info: {}'.format(test_auc(max_idx), random_settings(max_idx)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import metrics\n",
    "# random search for hyperparameter\n",
    "ntrial = 100\n",
    "train_errs, val_errs = [], []\n",
    "test_auc, test_rmse, test_acc = [], [], []\n",
    "random_settings = []\n",
    "\n",
    "\n",
    "for itrial in range(ntrial):\n",
    "    # grid search\n",
    "    # test_setting = test_settings[itrial]\n",
    "\n",
    "    # random search\n",
    "    print('random search {}/{}'.format(itrial, ntrial))\n",
    "    test_setting_1 = random.choice(test_settings_1)\n",
    "    test_setting_2 = random.choice(test_settings_2)\n",
    "    \n",
    "\n",
    "    # test_setting\n",
    "    num_l1, num_l2, num_l3, num_l4, kernel_l1, kernel_l2, kernel_l3, kernel_l4 = test_setting_1\n",
    "    dense_node, dropout_cnn, dropout_fc, globalpool_opt, batch_size, conv_double = test_setting_2\n",
    "\n",
    "        \n",
    "\n",
    "    # 이번 옵션에 대한 결과 디렉토리\n",
    "    odir_f = 'batch={}, c1={}, c2={}, c3={}, c4={}, c1filts={}, c2filts={}, c3filts={}, c4filts={}, conv_double={}, globalpool_opt={}, dropout={}, dnodes={}, dropout={}'.format(batch_size, num_l1, num_l2, num_l3, num_l4, kernel_l1, kernel_l2, kernel_l3, kernel_l4, conv_double, globalpool_opt, dropout_cnn, dense_node, dropout_fc)\n",
    "    random_settings.append(odir_f)\n",
    "    \n",
    "    odir = rootdir + '/' + odir_f\n",
    "    if not os.path.exists(odir):\n",
    "        os.mkdir(odir)\n",
    "\n",
    "    weightcache = \"{}/model.hdf5\".format(odir)        \n",
    "\n",
    "    strategy = tf.distribute.MirroredStrategy(devices=[\"/gpu:0\", \"/gpu:1\"])\n",
    "    with strategy.scope():\n",
    "        # build a model\n",
    "        model = Sequential()\n",
    "\n",
    "        conv_act = True\n",
    "        if conv_act:\n",
    "            act = 'relu'\n",
    "        else:\n",
    "            act = None\n",
    "\n",
    "        # c1 layer\n",
    "        if conv_double:\n",
    "            model.add(Conv1D(filters=num_l1, kernel_size=kernel_l1, padding='same'))\n",
    "        model.add(Conv1D(filters=num_l1, kernel_size=kernel_l1, padding='same', activation=act))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling1D(pool_size=pool_size))\n",
    "\n",
    "        # c2 layer\n",
    "        if conv_double:\n",
    "            model.add(Conv1D(filters=num_l2, kernel_size=kernel_l2, padding='same'))\n",
    "        model.add(Conv1D(filters=num_l2, kernel_size=kernel_l2, padding='same', activation=act))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling1D(pool_size=pool_size))\n",
    "        \n",
    "        \n",
    "        # c3 layer\n",
    "        if conv_double:\n",
    "            model.add(Conv1D(filters=num_l3, kernel_size=kernel_l3, padding='same'))\n",
    "        model.add(Conv1D(filters=num_l3, kernel_size=kernel_l3, padding='same', activation=act))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling1D(pool_size=pool_size))\n",
    "        \n",
    "        \n",
    "        # c4 layer\n",
    "        if conv_double:\n",
    "            model.add(Conv1D(filters=num_l4, kernel_size=kernel_l4, padding='same'))\n",
    "        model.add(Conv1D(filters=num_l4, kernel_size=kernel_l4, padding='same', activation=act))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling1D(pool_size=pool_size))\n",
    "\n",
    "\n",
    "        # global이냐 flatten이냐는 따로 모델 나눠야 할듯\n",
    "        if globalpool_opt == 'max':\n",
    "            model.add(GlobalMaxPool1D())\n",
    "        elif globalpool_opt == 'ave':\n",
    "            model.add(GlobalAveragePooling1D())\n",
    "            \n",
    "            \n",
    "        if dense_node != 0:\n",
    "            model.add(Dropout(dropout_cnn))\n",
    "            model.add(Dense(dense_node, activation='sigmoid'))\n",
    "        model.add(Dropout(dropout_fc))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "        # model 학습 설정\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=learning_rate), metrics=[\"mean_absolute_error\", tf.keras.metrics.AUC()])\n",
    "        hist = model.fit(x_train, y_train/10, validation_split=0.1, epochs=100, batch_size=batch_size, #class_weight={0:1, 1:3}, \n",
    "                                callbacks=[ModelCheckpoint(monitor='val_loss', filepath=weightcache, verbose=1, save_best_only=True),\n",
    "                                            EarlyStopping(monitor='val_loss', patience=5, verbose=0, mode='auto')])\n",
    "\n",
    "\n",
    "    # 모델의 아키텍처 및 구조 저장\n",
    "    open(odir+\"/model.json\", \"wt\").write(model.to_json())\n",
    "\n",
    "    # test set에 대한 y_pred 계산\n",
    "    model.load_weights(weightcache)  # fit 함수는 마지막 epoch의 결과를 리턴하기 때문에 best 결과를 다시 읽어들어야함\n",
    "    y_pred = model.predict(x_test).flatten()\n",
    "\n",
    "    # auroc 계산\n",
    "    false_positive_rate, true_positive_rate, threshold = roc_curve(y_test_bin, y_pred)\n",
    "    roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "    test_auc.append(roc_auc)\n",
    "    # RMSE 계산\n",
    "    model_err = metrics.RootMeanSquaredError()\n",
    "    model_err.update_state(y_test, y_pred)\n",
    "    rmse_val = model_err.result().numpy()\n",
    "    test_rmse.append(rmse_val)\n",
    "    # acc 계산\n",
    "    acc_val = np.mean((y_pred*10>=5)==y_test_bin)\n",
    "    test_acc.append(acc_val)\n",
    "    # rename\n",
    "    os.rename(odir, rootdir+'/auc{:.4f}_{}_rmse{:.4f}_acc{:.2f}'.format(roc_auc, odir_f, rmse_val, acc_val))\n",
    "\n",
    "    # train 과정에서의 err\n",
    "    train_err = min(hist.history['loss'])\n",
    "    val_err = min(hist.history['val_loss'])\n",
    "\n",
    "    val_errs.append(val_err)\n",
    "    train_errs.append(train_err)\n",
    "\n",
    "\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "\n",
    "max_idx = test_auc.index(max(test_auc))\n",
    "print('\\nBest Model roc:{:.4f}, info: {}'.format(test_auc(max_idx), random_settings(max_idx)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1D-CNN model 1\n",
    "[input]-(conv1-bn-maxpool)-(conv1-bn-maxpool)-(conv1-bn-maxpool)-(conv1-bn-maxpool)-(global maxpool)-dropout-(dense)-dropout-[output]\n",
    "<br>or conv-conv-bn-maxpool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T00:31:13.940731Z",
     "start_time": "2020-12-03T00:31:13.936119Z"
    }
   },
   "outputs": [],
   "source": [
    "max_idx = test_auc.index(max(test_auc))\n",
    "print('\\nBest Model roc:{:.4f}\\ninfo: {}'.format(test_auc[max_idx], random_settings[max_idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 10 Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T00:31:09.428287Z",
     "start_time": "2020-12-03T00:31:09.408817Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# top 10 model\n",
    "topid= sorted(range(len(test_auc)),key= lambda i: test_auc[i])[-10:]\n",
    "\n",
    "for i in range(10):\n",
    "    print('Top {} Model: roc {:.4f}   train mse {:.4f}  val mse {:4f}'.format(i+1, np.array(test_auc)[topid[9-i]], np.array(train_errs)[topid[9-i]], np.array(val_errs)[topid[9-i]]))\n",
    "    print(' {}\\n'.format(np.array(random_settings[topid[9-i]])))\n",
    "\n",
    "    #np.array(test_auc)[topid], np.array(random_settings)[topid]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1D-CNN model 2\n",
    "[input]-(conv1-bn-maxpool)-(conv1-bn-maxpool)-(global max or ave or flatten)-dropout-(dense)-dropout-[output]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-27T02:02:04.577729Z",
     "start_time": "2020-11-27T02:02:04.573394Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_idx = test_auc.index(max(test_auc))\n",
    "print('\\nBest Model roc:{:.4f}  rmse:{:.4f}  acc:{:.2f}\\n: {}'.format(test_auc[max_idx], test_rmse[max_idx], test_acc[max_idx], random_settings[max_idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-27T02:02:42.552172Z",
     "start_time": "2020-11-27T02:02:42.529164Z"
    }
   },
   "outputs": [],
   "source": [
    "# top 10 model\n",
    "topid= sorted(range(len(test_auc)),key= lambda i: test_auc[i])[-10:]\n",
    "\n",
    "for i in range(10):\n",
    "    print('Top {} Model: roc {:.4f}  rmse:{:.4f}  acc:{:.2f},  train mse {:.4f}  val mse {:4f}'.format(i+1, np.array(test_auc)[topid[9-i]],np.array(test_rmse)[topid[9-i]], np.array(test_acc)[topid[9-i]], np.array(train_errs)[topid[9-i]], np.array(val_errs)[topid[9-i]]))\n",
    "    print(' {}\\n'.format(np.array(random_settings[topid[9-i]])))\n",
    "\n",
    "    #np.array(test_auc)[topid], np.array(random_settings)[topid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(num_l1=256, kernel_l1 = 10, bool_flatten=False, dropout=0.2):\n",
    "    #strategy = tf.distribute.MirroredStrategy(devices=[\"/gpu:0\", \"/gpu:1\"])\n",
    "    #with strategy.scope():\n",
    "    config = tf.compat.v1.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    sess = tf.compat.v1.Session(config=config)\n",
    "    model = Sequential()\n",
    "    #num_l2=64, num_l3=64, kernel_l2=3, kernel_l3=3\n",
    "    num_l2=64\n",
    "    num_l3=64\n",
    "    kernel_l2=3\n",
    "    kernel_l3=3\n",
    "\n",
    "    #for (num_node, kernel_size) in conv_layers:\n",
    "    #    model.add(Conv1D(filters=num_node, kernel_size=kernel_size, padding='valid'))\n",
    "    #    model.add(BatchNormalization())\n",
    "    #    model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "    # Conv Layer 1\n",
    "    model.add(Conv1D(filters=num_l1, kernel_size=kernel_l1, padding='valid'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling1D(pool_size=2)) \n",
    "\n",
    "    # Conv Layer 2\n",
    "    model.add(Conv1D(filters=num_l2, kernel_size=kernel_l2, padding='valid'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling1D(pool_size=2)) \n",
    "\n",
    "    # Conv Layer 3\n",
    "    model.add(Conv1D(filters=num_l3, kernel_size=kernel_l3, padding='valid'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling1D(pool_size=2)) \n",
    "\n",
    "    # FC layer 이전의 작업\n",
    "    if bool_flatten:\n",
    "        model.add(Flatten())\n",
    "    else:\n",
    "        model.add(GlobalMaxPool1D())\n",
    "\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.001), metrics=['accuracy'])\n",
    "\n",
    "\n",
    "return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "228.479px",
    "left": "1473.25px",
    "right": "20px",
    "top": "171px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
