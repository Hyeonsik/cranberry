{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train, test loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-01T03:49:49.600156Z",
     "start_time": "2020-12-01T03:49:11.155008Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading train...done\n",
      "x_train shape: (131302, 5000, 2)\n",
      "x_test.shape: (13749,)\n"
     ]
    }
   ],
   "source": [
    "dataset_path = 'dataset/preprocess2/input3-2/'\n",
    "\n",
    "print('loading train...', flush=True, end='')\n",
    "\n",
    "# x를 loading해서 (batch_size, step, channel)\n",
    "x_train = np.load(dataset_path+'x_train.npz', allow_pickle=True)['arr_0']\n",
    "x_test = np.load(dataset_path+'x_test.npz', allow_pickle=True)['arr_0']\n",
    "y_train = np.load(dataset_path+'y_train.npz')['arr_0']\n",
    "y_test = np.load(dataset_path+'y_test.npz')['arr_0']\n",
    "print('done', flush=True)\n",
    "\n",
    "# binary classification\n",
    "y_train_bin = y_train >= 5\n",
    "y_test_bin = y_test >= 5\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test.shape:', y_test.shape)\n",
    "\n",
    "\n",
    "# random shuffling\n",
    "ids = np.arange(x_train.shape[0])\n",
    "np.random.shuffle(ids)\n",
    "\n",
    "x_train = x_train[ids]\n",
    "y_train = y_train[ids]\n",
    "y_train_bin = y_train_bin[ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-01T05:27:36.166Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.models import Model, load_model\n",
    "from keras.optimizers import Adam as Adam\n",
    "from keras.layers import Dense, Conv1D, MaxPooling1D, GlobalMaxPool1D, BatchNormalization, Dropout, Activation\n",
    "from keras.layers import GlobalAveragePooling1D, Flatten, SeparableConv1D\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from sklearn.metrics import auc, classification_report, confusion_matrix, accuracy_score, roc_curve, roc_auc_score, f1_score, precision_recall_curve\n",
    "import tensorflow as tf\n",
    "import os, pickle\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "# coefficient of determination (R^2) for regression  (only for Keras tensors)\n",
    "def r_square(y_true, y_pred):\n",
    "    SS_res =  K.sum(K.square(y_true - y_pred)) \n",
    "    SS_tot = K.sum(K.square(y_true - K.mean(y_true))) \n",
    "    return ( 1 - SS_res/(SS_tot + K.epsilon()) )\n",
    "\n",
    "\n",
    "# hyperparamters\n",
    "num_nodes = [64, 64,64,64] #, 64, 64, 64]\n",
    "kernel_size = 10\n",
    "pool_size = 2\n",
    "BATCH_SIZE = 512\n",
    "dense_node = 0\n",
    "dropout_rate = 0.3\n",
    "dropout_cnn = 0.2\n",
    "dropout_fc = 0.2\n",
    "learning_rate = 0.002\n",
    "\n",
    "\n",
    "testname = '-'.join([str(num_node) for num_node in num_nodes])\n",
    "print(testname)\n",
    "\n",
    "# 출력 폴더를 생성\n",
    "model_name = 'model_reg_'\n",
    "for num_node in num_nodes:\n",
    "    model_name += '{}_'.format(num_node)\n",
    "model_name += 'filtersize{}_relu_bn_maxpool{}_globalmaxpool_dropout{}_dense{}_dropout{}_batch{}_lr_{}'.format(kernel_size, pool_size, dropout_cnn, dense_node, dropout_fc, BATCH_SIZE, learning_rate)\n",
    "\n",
    "#model_name = 'model_reg_{}_{}_{}_size{}_relu_bn_maxpool{}_globalmaxpool_dense32_dropout{}_batch{}_learning_rate{}'.format(num_nodes[0], num_nodes[1], num_nodes[2], kernel_size, pool_size, dropout_rate, BATCH_SIZE, learning_rate)\n",
    "save_path = \"output/1D_CNN_\"+model_name\n",
    "\n",
    "if not os.path.exists(save_path):\n",
    "    os.mkdir(save_path)\n",
    "weight_path = save_path + \"/weights.hdf5\"\n",
    "\n",
    "\n",
    "# GPU 설정\n",
    "strategy = tf.distribute.MirroredStrategy(devices=[\"/gpu:1\"])\n",
    "with strategy.scope():\n",
    "    \n",
    "    # build a model\n",
    "    model = Sequential()\n",
    "    for num_node in num_nodes:\n",
    "        model.add(Conv1D(filters=num_node, kernel_size=kernel_size, padding='valid', activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling1D(pool_size=pool_size))\n",
    "    #model.add(BatchNormalization())    \n",
    "    model.add(GlobalMaxPool1D())\n",
    "    #model.add(Flatten())\n",
    "    model.add(Dropout(dropout_cnn))\n",
    "    #model.add(Activation('sigmoid'))\n",
    "    #model.add(Dropout(dropout_rate))\n",
    "    if dense_node != 0:\n",
    "        model.add(Dense(dense_node, activation='tanh'))\n",
    "        model.add(Dropout(dropout_fc))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    \n",
    "    # model 학습 설정\n",
    "    model.compile(loss='mse', optimizer=Adam(lr=learning_rate), metrics=[\"mean_absolute_error\"])\n",
    "    hist = model.fit(x_train, y_train/10, validation_split=0.1, epochs=100, batch_size=BATCH_SIZE, #class_weight={0:1, 1:3}, \n",
    "                            callbacks=[ModelCheckpoint(monitor='val_loss', filepath=weight_path, verbose=1, save_best_only=True),\n",
    "                                        EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='auto')])\n",
    "\n",
    "    #tf.keras.backend.clear_session()\n",
    "\n",
    "model.load_weights(weight_path)\n",
    "    \n",
    "# 모델의 아키텍처 및 구조 저장\n",
    "open(save_path + \"/model.json\", \"wt\").write(model.to_json())\n",
    "\n",
    "# 전체 test 샘플을 한번에 예측\n",
    "y_pred = model.predict(x_test).flatten()\n",
    "\n",
    "# 결과를 저장\n",
    "#np.savetxt(save_path+'/pred_y.txt', y_pred)\n",
    "\n",
    "\n",
    "# 모델의 history log 저장 - binary classification\n",
    "for key in hist.history.keys():\n",
    "    if 'auc' in key and not 'val' in key:\n",
    "        auc_key = key\n",
    "#pickle.dump((hist.history['loss'], hist.history['val_loss'], hist.history['accuracy'], hist.history['val_accuracy'], hist.history[auc], hist.history['val_'+auc]), open(save_path+'/history', 'wb'))\n",
    "\n",
    "# 모델의 history log 저장 - regression\n",
    "pickle.dump((hist.history['loss'], hist.history['val_loss'], hist.history['mean_absolute_error'], hist.history['val_mean_absolute_error']), open(save_path+'/history', 'wb'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc, r2_score\n",
    "from numpy import interp\n",
    "from keras import losses, metrics\n",
    "import keras.backend as K\n",
    "\n",
    "### Classification\n",
    "# Model Accuracy of test set\n",
    "#model_y = np.where(y_pred<0.5,0,1)\n",
    "#print('test set accuracy:{:.2f}'.format(np.mean(model_y==y_test_bin)))\n",
    "\n",
    "### Regression\n",
    "# Model MSE of test set\n",
    "#mse_val = K.eval(losses.mean_squared_error(y_test, y_pred))\n",
    "model_err = metrics.RootMeanSquaredError()\n",
    "model_err.update_state(y_test, y_pred)\n",
    "rmse_val = model_err.result().numpy()\n",
    "acc_val = np.mean((y_pred*10>=5)==y_test_bin)\n",
    "print('test set mse:{:.2f}'.format(rmse_val))\n",
    "print('test set accuracy:{:.2f}'.format(acc_val))\n",
    "\n",
    "\n",
    "# Model AUROC\n",
    "false_positive_rate, true_positive_rate, threshold = roc_curve(y_test_bin, y_pred)\n",
    "roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "print('test set auroc:', roc_auc)\n",
    "\n",
    "# Model R_square\n",
    "#print('test set R2:', r2_score(y_test, y_pred*9))\n",
    "\n",
    "\n",
    "# Adding evaluation results to file name\n",
    "# classification\n",
    "#os.rename(save_path, save_path+'_auc{:.4f}_acc{:.4f}'.format(roc_auc,np.mean(model_y==y_test_bin)))\n",
    "\n",
    "# regression\n",
    "os.rename(save_path, \"output/auc{:.4f}_1D_CNN_{}rmse{:.4f}_acc{:.2f}\".format(roc_auc, model_name, rmse_val,acc_val))\n",
    "\n",
    "\n",
    "# plotting roc\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.xlabel(\"False Positive Rate(1 - Specificity)\")\n",
    "plt.ylabel('True Positive Rate(Sensitivity)')\n",
    "\n",
    "plt.plot(false_positive_rate, true_positive_rate, 'b', label='Model 1 (AUC = %0.4f)'% roc_auc)\n",
    "plt.plot([0,1],[1,1],'y--')\n",
    "plt.plot([0,1],[0,1],'r--')\n",
    "\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model history plot\n",
    "- training curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, loss_ax = plt.subplots(figsize=(20,10))\n",
    "\n",
    "#x-axis는 공유하지만 y-axis는 공유x\n",
    "acc_ax = loss_ax.twinx()\n",
    "\n",
    "loss_ax.plot(hist.history['loss'], 'y', linestyle='dashed', label='train loss')\n",
    "loss_ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "\n",
    "acc_ax.plot(hist.history['mean_absolute_error'], 'b', linestyle='dashed',label='train mae')\n",
    "acc_ax.plot(hist.history['val_mean_absolute_error'], 'g', label='val mae')\n",
    "#acc_ax.plot(hist.history[auc_key], 'b', linestyle='dashed',label='train auc')\n",
    "#acc_ax.plot(hist.history['val_'+auc_key], 'g', label='val auc')\n",
    "\n",
    "\n",
    "\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "#acc_ax.set_ylim(0.2,1.0)\n",
    "acc_ax.set_ylabel('accuray')\n",
    "\n",
    "loss_ax.legend(loc='upper left')\n",
    "acc_ax.legend(loc='lower left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
